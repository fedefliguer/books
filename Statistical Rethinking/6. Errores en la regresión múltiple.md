## 6. EL DAG EMBRUJADO Y EL TERROR CAUSAL

* La regresión múltiple es peligrosa. Imaginemos que en un panel de revisión de artículos periodísticos se reciben 200 propuestas. Cada una tiene su confiabilidad (rigor, erudición, plausibilidad de éxito) y su noticiabilidad (valor de bienestar social, interés público). Se clasifican las propuestas por sus puntajes combinados, y se selecciona el 10% superior para fondos. Entre ese mejor 10% se traza una línea de regresión, que puede mostrar una relación negativa. Y esto por qué sucede? Pues porque si la única forma de cruzar el umbral es obtener una puntuación alta, es más probable obtener una puntuación alta en un ítem que en los dos. La conclusión sería que los estudios más confiables son menos interesantes, pero es lo que se conoce como distorsión de selcción, o paradoja de Berkson. Este efecto puede ocurrir dentro de una regresión múltiple, porque el acto de agregar un predictor induce una selección estadística dentro del modelo, un fenómeno que se conoce con el nombre de sesgo del colisionador. Este capítulo explorará tres peligros distintos: la multicolinealidad, el sesgo posterior al tratamiento y el sesgo del colisionador.

### 6.1. Multicolinealidad

* La multicolinealidad significa una correlación muy fuerte entre dos o más variables predictoras. La consecuencia de esto es que la distribución posterior parecerá sugerir que ninguna de las variables está asociada de manera confiable con el resultado, incluso si todas las variables están en realidad fuertemente asociadas.
* Por ejemplo, buscaremos predecir la altura de un individuo utilizando la longitud de sus piernas como variables predictoras. Seguramente la altura se asocia positivamente con la longitud de la pierna, o al menos la simulación asumirá que lo es. Sin embargo, una vez que pones ambos longitudes de las piernas en el modelo, sucederá algo molesto.
```r
m6.1 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + bl*leg_left + br*leg_right ,
a ~ dnorm( 10 , 100 ) ,
bl ~ dnorm( 2 , 10 ) ,
br ~ dnorm( 2 , 10 ) ,
sigma ~ dexp( 1 )
) ,
data=d )
precis(m6.1)

mean sd 5.5% 94.5%
a 0.98 0.28 0.53 1.44
bl 0.21 2.53 -3.83 4.25
br 1.78 2.53 -2.26 5.83
sigma 0.62 0.04 0.55 0.69
```
* Si ambas piernas tienen longitudes casi idénticas, y la altura está tan fuertemente asociada con la longitud de la pierna, entonces ¿qué es esta distribución posterior tan extraña? ¿Funcionó correctamente la aproximación posterior? Funcionó correctamente, y la distribución posterior aquí es la respuesta correcta a la pregunta que hicimos. El problema es la pregunta. Recuerda que una regresión lineal múltiple responde la pregunta: ¿Cuál es el valor de conocer cada predictor, después de conocer ya todos los otros predictores? En este caso, la pregunta se convierte en: ¿Cuál es el valor de conocer cada longitud de la pierna, después de saber la longitud de la otra pierna? La respuesta a esta extraña pregunta es igualmente extraña, pero perfectamente lógica. Si se ajusta una regresión solo con una de las piernas, el coeficiente será muy cercano a 2, que es la suma de bl + br.
* El problema de la multicolinealidad es un miembro de una familia de problemas en el ajuste de modelos, llamados problemas de identificación. Cuando un parámetro no es identificable, significa que la estructura de los datos y el modelo no permiten estimar el valor del parámetro. A veces, este problema surge de errores en el código de un modelo, pero muchos tipos importantes de modelos presentan parámetros no identificables o débilmente identificables, incluso cuando el código está bien. La naturaleza no nos debe una inferencia fácil, aun cuando el modelo es correcto. En general, no hay garantía de que los datos disponibles contengan mucha información sobre un parámetro de interés. Cuando eso es cierto, el modelo bayesiano devolverá una distribución del posterior muy similar al prior. Por lo tanto, comparar prior con posterior puede ser una buena idea, una forma de ver cuánta información extrajo el modelo de los datos. Cuando prior y posterior son similares, no significa que los cálculos sean incorrectos: usted obtuve la respuesta correcta a la pregunta que hiciste.

### 6.2. Sesgo post-tratamiento

* Supongamos, por ejemplo, que estás cultivando algunas plantas en un invernadero. Quieres saber la diferencia en el crecimiento bajo diferentes tratamientos antifúngicos del suelo, porque el hongo en el plantas tiende a reducir su crecimiento. Las plantas inicialmente se siembran y brotan. sus alturas son medidas. Luego se aplican diferentes tratamientos al suelo. Hay cuatro variables de interés: altura inicial, altura final, tratamiento y presencia de hongos. La altura final es el resultado de interés. Pero cuál de las otras variables debe estar en el modelo? Si su objetivo es hacer una inferencia causal sobre el tratamiento, no debe incluir la presencia de hongos, porque es un efecto post-tratamiento.
* Sabemos que las plantas en el tiempo t = 1 deberían ser más altas que en el tiempo t = 0, cualquiera que sea la escala. Entonces, si ponemos los parámetros en una escala de proporción de altura en el tiempo t = 0, en lugar de en la escala absoluta de los datos, podemos establecer los priors más fácilmente. $h_{1,i}$ se distribuye con una normal de media $\mu_{i}$ y varianza $\sigma$. a su vez $\mu_{i}$ es igual a $h_{0,i} * p$. Podemos definir $h_{0,i}$ como la altura de la planta en el momento 0, $h_{1,i}$ como la altura en el momento 1, y p como un parámetro, siempre positivo y esperablemente mayor a 1, que mide la proporción. Si se regresiona de esa manera, queda una media de 1.43 para p, es decir una proproción esperable de 43%. Si en cambio se regresiona incorporando la variable de tratamiento y la de los hongos, aparece que el alfa es de 1.43 y el beta del tratamiento es estrictamente 0, no estando relacionado con el crecimiento. Nosotros sabemos que sí lo está. El problema es que los hongos son principalmente una consecuencia del tratamiento. Es decir, el hongo es una variable post-tratamiento. Así que cuando controlamos para los hongos, el modelo responde implícitamente a la pregunta: una vez que ya sabemos si o no una planta desarrolló un hongo, ¿importa el tratamiento del suelo? La respuesta es "no". Si en cambio se regresiona usando como única variable explicativa el tratamiento, el alfa queda 1.38 y el beta 0.08, explicando el crecimiento mejor.

### 6.3. Sesgo del colisionador

* Imaginemos que cada año 20 personas nacen, con distribuciones uniformes de felicidad. Todos ellos cada año crecen un año, y su felicidad no cambia. A los 18 años, las personas pueden casarse, con probabilidades proporcionales a su felicidad. Una vez que se casan, los individuos permanencen casados hasta los 65 años. Tenemos tres variables entonces: age, married y happiness. Si se quiere saber si la edad está realacionada con la felicidad, una regresión múltiple nos encontrará con este sesgo. El colisionador es el estado civil, que es una consecuencia común de la edad y la felicidad. Como resultado, entre las dos causas se podrá encontrar una asociación espuria. Una vez que sabemos si alguien está casado o no, entonces su edad proporciona información sobre lo felices que son. Solo entre los puntos azules pareciera haber una relación negativa donde las personas mayores tienen menos felicidad. Solo entre los puntos grises también parece haber esa relación: la felicidad baja con los años. Sin embargo, ninguna refleja la asociación causal, que por medio de la variable del matrimonio es positiva.
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/6.1.png"> </p>
* Otro ejemplo gráfico. Entre la máxima educación de los abuelos y la educación máxima de los nietos parece haber una relación negativa. El inobservable 'tipo de barrio' esconde que se trata de una relación positiva cuando se condiciona por esta característica importante. En ambos grupos, cuando se condiciona la relación se vuelve negativa. Ambos son ejemplos de la paradoja de Simpson: incluir otro predictor puede revertir la dirección de asociación entre algún otro predictor y el resultado. En general esta paradoja se presenta en los casos donde añadir el nuevo predictor ayuda, es decir que permite encontrar la verdadera relación. Pero a veces ese predictor es lo que está engañando. Para saber si la relación refleja correctamente la causalidad, se necesita más que un modelo estadístico.
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/6.2.png"> </p>
