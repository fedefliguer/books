## 5. MUCHAS VARIABLES Y WAFFLES ESPURIOS

* En grandes conjuntos de datos, cada par de variables tiene una correlación distinta de cero con el resto. Como la mayoría de las correlaciones no indican relaciones causales, necesitamos herramientas para distinguir mera asociación de evidencia de causalidad. Es por eso que se dedica tanto esfuerzo estadístico a la regresión multivariada. La necesidad de usar este tipo de modelos es clara: los eventos, en la práctica, suelen tener muchas causas. Pero además, considerar una única causa puede ser peligroso por la llamada Paradoja de Simpson, donde la dirección de una asociación entre dos variables sería distinta en caso de incluir una tercera. Por último, es importante considerar la probabilidad de no linealidades, es decir que las causas produzcan las consecuencias a través de interacciones.

### 5.1. Regresión multivariada

* El criterio para la regresión multivariada consiste en nominar las variables posibles predictoras, para cada una generar un parámetro, y luego añadir todos al modeo lineal. Para cada parámetro existiría un valor estimado con su desviación estándar, y podrán generarse los intervalos 5.5% y 94.5%.
* Existen diferentes formas de visualizar la interpretación de esta regresión múltiple: residuos (tomar una de las variables explicativas y la explicada, y en forma bivariada graficar la distancia respecto del valor proyectado), contrafácticos (cómo cambia el valor esperado de la explicada si cambia una explicativa, dejando las otras en su media), o gráfico del posterior (sea observación por observación o por grupos, esperado vs efectivo o proemdio esperado vs promedio efectivo)
* Puede suceder que agregar dos variables explicativas conduzca a un mayor efecto en ambos, que cada uno por su lado. Estos son los casos en donde hay dos variables correlacionadas con el resultado, pero una se correlaciona positivamente con él y la otra se correlaciona negativamente con él. Además, ambas variables explicativas están positivamente correlacionadas entre sí. Como
Como resultado, tienden a cancelarse entre sí. Este es otro caso en el que la regresión automáticamente encuentra los casos más reveladores y los usa para producir estimaciones.

### 5.3. Cuando agregar variables perjudica

* Diferentes casos pueden exponer los problemas que puede tener la incorporación de variables:
1. La multicolinealidad significa una correlación muy fuerte entre dos o más variables predictoras. La consecuencia de esto es que la distribución posterior dirá que un rango muy grande de valores de parámetros es plausible, desde pequeñas asociaciones hasta masivas, incluso si todas las variables están en realidad fuertemente asociadas con la variable de resultado. Cuando dos variables predictoras están muy fuertemente correlacionadas, ambas en un modelo pueden generar confusión. La distribución posterior no está mal, en tales casos. Si solo estás interesado en la predicción, encontrarás que este modelo hace buenas predicciones. Las diferentes disciplinas tienen diferentes convenciones para tratar con variables colineales. En algunos campos, es típico participar en algún tipo de procedimiento de reducción de datos, como componentes principales o análisis factorial, y luego usar los componentes / factores como predictor.
2. El sesgo post-tratamiento tiene que ver con incluir variables que son consecuencia de otras variables. Tiene sentido controlar
las diferencias previas al tratamiento, incluir variables posteriores al tratamiento en realidad puede enmascarar el tratamiento en sí. El problema de las variables posteriores al tratamiento se aplica tan bien a los estudios observacionales como lo hace a los experimentos. Pero en los experimentos puede ser fácil saber qué variables son pretratamiento y cuáles son post-tratamiento, mientras que en los observacionales menos.
