## 4. MODELOS LINEALES

* :gem: La teoría ptolomeica de la tierra decía que el sistema solar era geocéntrico, cosa por supuesto falsa, pero sin embargo la estrategia que lo justificó (el uso de epiciclos, el verdadero aporte de ptolomeo) resulta un sistema generalizado de aproximación que hace buenas predicciones.
* Por "regresión lineal" nos referiremos a una familia de golems estadísticos simples que intentan aprender sobre la media y varianza de alguna medida, usando una combinación aditiva de otras medidas. Como el geocentrismo, la regresión lineal puede describir útilmente una gran variedad de fenómenos naturales.

### 4.1. Por qué las distribuciones normales son normales

* La mayoría de los procesos de la naturaleza no son gaussianos o normales. Sin embargo, cualquier proceso que sigue una distribución de probabilidad tiene un promedio, y cualquiera sea el promedio, cada muestra de la distribución puede considerarse como una fluctuación sobre ese valor promedio. Cuando comenzamos a sumar estas fluctuaciones, también comienzan a cancelarse entre sí. Una gran fluctuación positiva cancelará una gran negativa. Cuantos más términos en la suma, más posibilidades de que cada fluctuación sea cancelada por otra o por una serie de otras más pequeñas en la dirección opuesta. 
* El ejemplo típico podría ser un dado (distribución uniforme) pero el promedio de dos dados ya no es uniforme: hay muchas más chances de promedio 3 (4+2, 2+4, 3+3, 1+5, 5+1) que de promedio 6 o promedio 1. Otros efectos a través de la multiplicación o la log-multiplicación también se convierten en normales.
* Entonces, si bien la mayoría de los procesos de la naturaleza no son gaussianos o normales en esencia, muchos provienen de sumas o multiplicacioones de procesos no gaussianos, y se transfomran en gaussianos. Errores de medición, variaciones en el crecimiento y velocidades de moléculas podrían ser ejemplos de esto.
* Las conveniencias de la distribución gaussiana no son únicamente la abundancia en procesos, sino también una conveniencia epistemológica, dado que esta distribución solo necesita una media y una varianza. Es la expresión máxima de la ignorancia, porque es consistente con los supuestos mínimos: si estamos dispuestos a aceptar que nuestra distribución tiene varianza finita, la gaussiana es la más probable. Más bien, para suponer que la distribución no es gaussiana tendríamos que tener información adicional. Esto volverá a aparecer cuando se vea la teoría de la información.
* Sobre la distribución normal: la fórmula es compleja, pero el factor más importante es (y-μ)^2 que da a la distribución una forma cuadrática. El resto de la fórmula únicamente estandariza esto. Además, es la primera de las distribuciones que vemos que es continua, por lo que no tiene una función de masa de probabilidad sino una función de densidad. La densidad de probabilidad es la tasa de cambio en la probabilidad acumulativa, y entonces puede ser mayor a 1. Lo que nunca podrá ser mayor a 1 es el área bajo la curva.

### 4.2. Un lenguaje para describir modelos

* Las decisiones que forman parte de un modelo de este tipo son varias: el conjunto de medidas que esperamos predecir o comprender, es decir el resultado; la distribución de probabilidad de cada observación individual (la pensaremos normal en regresión lineal); cuáles son las variables predictoras; la forma en la que los predictores se relacionan con el resultado; y los priors de los parámetros del modelo.
* Esto es otro lenguaje con el que los problemas pueden describirse. El caso del agua en la tierra, por ejemplo, podría escribirse como un valor w que se distribuye en una biomial(n,p), con p con un prior uniforme entre 0 y 1. Una vez que conocemos el modelo de esta manera, conocemos automáticamente todos sus supuestos, ya que sabemos que la distribución binomial supone que cada muestra (lanzamiento de globo) es independiente de la otra, y también sabemos que el modelo supone que los puntos de muestra son independientes entre sí.

### 4.3. Un modelo gaussiano de altura

* Hay un número infinito de posibles distribuciones gaussianas: algunas tienen medias más chicas o más grandes, varianzas más chicas o más grandes. Para construir un modelo, queremos que nuestra máquina bayesiana considere todas las distribuciones posibles, y clasificarlas por plausibilidad posterior, que es la compatibilidad lógica de cada posible distribución con los datos y el modelo.
```r
library(rethinking) 4.7
data(Howell1) # 544 individuos con su altura (en centímetros), su peso (en kilos), su edad y si son hombres o mujeres.
d <- Howell1
d2 <- d[d$age>=18,] # Filtro adultos
```
* Si nos concentramos únicamente en la altura, vemos que la densidad tiene una forma similar a la normal. Como se trata de una normal, podría ser una acumulación de distribuciones subyacentes, como ya vimos. Pero usemos la distribución normal: ¿cuál hay que usar? necestamos una combinación media-varianza, que no tenemos aún. Por eso escribimos hi~Normal(μ; σ), asumiendo iid: asumiendo que todos los valores tienen igual probabilidad. Es una suposición que muchas veces no parece probable en el mundo físico, pero sí en dentro del modelo.
* Para completar el modelo es necesario generar los priors de nuestros parámetros. μ tendrá una distribución normal(178,20) y σ una uniforme(0,50), definidas en forma arbitraria. Con eso ya podemos calcular, por medio de la aproximación de grilla, la distribución posterior.
```r
flist <- alist( 4.25
height ~ dnorm( mu , sigma ) ,
mu ~ dnorm( 178 , 20 ) ,
sigma ~ dunif( 0 , 50 )
)

m4.1 <- map( flist , data=d2 )
precis( m4.1 )

Mean StdDev 5.5% 94.5%
mu 154.61 0.41 153.95 155.27
sigma 7.73 0.29 7.27 8.20
```

### 4.4. Agregando un predictor

* Hasta ahora lo que hicimos no fue estrictamente una regresión, ya que simplemente generamos un modelo gaussiano para la altura en una población de adultos. La regresión obviamente no es eso, sino que tiene que ver con la información de las variables. El concepto de 'regresión a la media' lo acuñó Galton, al observar una contracción a medida que cada medición informa a las demás. Predecir la altura de un hijo teniendo la de su padre es un problema: predecir la relación entre la altura de los padres con la de los hijos teniendo muchos es más sencillo.
* La estrategia, llamada modelo lineal, es instruir al golem para que asuma que la variable predictora tiene una relación constante y aditiva con la media del resultado. El golem luego calcula la distribución posterior de esta relación constante. μi = α + βi. μ depende linealmente de dos parámetros, por lo que está implícitamente paramaetrizado. El nuevo parámetro β tendrá su prior, también. No hay nada de especial respecto de la relación lineal, que también podría ser de otra forma (por ejemplo exponencial).

```r
m4.3 <- map(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*weight ,
a ~ dnorm( 156 , 100 ) ,
b ~ dnorm( 0 , 10 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )

Mean StdDev 5.5% 94.5% a b sigma
a 154.60 0.27 154.17 155.03 1 0 0
b 0.91 0.04 0.84 0.97 0 1 0
sigma 5.07 0.19 4.77 5.38 0 0 1
```

* El resultado es el conocido: la estimación de los parámetros con su media, su desviación estándar y su 5% y 95%. Con estos resultados podría graficarse la línea, con los intervalos en los que podría estar. También podría condicionarse por un valor del predictor, graficando la campana de valores del posterior que se genera.
* Más allá de la estimación del parámetro que nos interesa, podríamos generar intervalos de confianza para los valores reales de las observaciones, que gráficamente serían bandas en torno a la media, más amplias que las del propio desvío estandar. Estas bandas incoporan la incertidumbre respecto de las predictoras, que hemos descripto en la primera linea del alist().
* Ajustar un modelo lineal, como se dijo, no es la única forma de proceder en estos casos. Aun con un solo predictor, podría ajustarse un modelo polinómico, que tiene más de un término: la misma variable en forma lineal y cuadrática, por ejemplo, o en forma lineal, cuadrática y cúbica. El procedimiento en R sería:

```r
d$weight.s <- ( d$weight - mean(d$weight) )/sd(d$weight) # Estandarizo la variable antes de ajustar 
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b1*weight.s + b2*weight.s2 ,
a ~ dnorm( 178 , 100 ) ,
b1 ~ dnorm( 0 , 10 ) ,
b2 ~ dnorm( 0 , 10 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d )

Mean StdDev 5.5% 94.5%
a 146.66 0.37 146.07 147.26
b1 21.40 0.29 20.94 21.86
b2 -8.42 0.28 -8.87 -7.97
sigma 5.75 0.17 5.47 6.03
```

### 4.5. Resumen

* Este capítulo introdujo el modelo de regresión lineal simple, un marco para estimar la asociación entre una variable predictiva y una variable de resultado. La distribución gaussiana comprende la probabilidad en tales modelos, porque cuenta los números relativos de
maneras diferentes combinaciones de medias y desviaciones estándar pueden producir una observación. El siguiente capítulo amplía estos conceptos al introducir modelos de regresión con más de una variable predictiva.
