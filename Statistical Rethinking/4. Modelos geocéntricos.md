## 4. MODELOS GEOCÉNTRICOS

* :gem: *La teoría ptolomeica de la tierra decía que el sistema solar era geocéntrico, cosa por supuesto falsa, pero sin embargo la estrategia que lo justificó (el uso de epiciclos, el verdadero aporte de ptolomeo) resulta un sistema generalizado de aproximación que hace buenas predicciones.*
* Por **"regresión lineal"** nos referiremos a una familia de golems estadísticos simples que intentan aprender sobre la media y varianza de alguna medida, usando una combinación aditiva de otras medidas. Como el geocentrismo, la regresión lineal puede describir útilmente una gran variedad de fenómenos naturales.

### 4.1. Por qué las distribuciones normales son normales

* La mayoría de los procesos de la naturaleza no son gaussianos o normales. Sin embargo, cualquier proceso que sigue una distribución de probabilidad tiene un promedio, y cualquiera sea el promedio, cada muestra de la distribución puede considerarse como una fluctuación sobre ese valor promedio. Cuando comenzamos a sumar estas fluctuaciones, también comienzan a cancelarse entre sí. Una gran fluctuación positiva cancelará una gran negativa. Cuantos más términos en la suma, más posibilidades de que cada fluctuación sea cancelada por otra o por una serie de otras más pequeñas en la dirección opuesta. 
* El ejemplo típico podría ser un dado (distribución uniforme) pero el promedio de dos dados ya no es uniforme: hay muchas más chances de promedio 3 (4+2, 2+4, 3+3, 1+5, 5+1) que de promedio 6 o promedio 1. Otros efectos a través de la multiplicación o la log-multiplicación también se convierten en normales.
* Entonces, si bien la mayoría de los procesos de la naturaleza no son gaussianos o normales en esencia, muchos provienen de sumas o multiplicacioones de procesos no gaussianos, y se transfomran en gaussianos. Errores de medición, variaciones en el crecimiento y velocidades de moléculas podrían ser ejemplos de esto.
* Las conveniencias de la distribución gaussiana no son únicamente la abundancia en procesos, sino también una conveniencia epistemológica, dado que esta distribución solo necesita una media y una varianza. Es la expresión máxima de la ignorancia, porque es consistente con los supuestos mínimos: si estamos dispuestos a aceptar que nuestra distribución tiene varianza finita, la gaussiana es la más probable. Más bien, para suponer que la distribución no es gaussiana tendríamos que tener información adicional. Esto volverá a aparecer cuando se vea la teoría de la información.
* Sobre la distribución normal: la fórmula es compleja, pero el factor más importante es (y-μ)^2 que da a la distribución una forma cuadrática. El resto de la fórmula únicamente estandariza esto. Además, es la primera de las distribuciones que vemos que es continua, por lo que no tiene una función de masa de probabilidad sino una función de densidad. La densidad de probabilidad es la tasa de cambio en la probabilidad acumulativa, y entonces puede ser mayor a 1. Lo que nunca podrá ser mayor a 1 es el área bajo la curva.

### 4.2. Un lenguaje para describir modelos

* Las decisiones que forman parte de un modelo de este tipo son varias: el conjunto de medidas que esperamos predecir o comprender, es decir el resultado; la distribución de probabilidad de cada observación individual (la pensaremos normal en regresión lineal); cuáles son las variables predictoras; la forma en la que los predictores se relacionan con el resultado; y los priors de los parámetros del modelo.
* Esto es otro lenguaje con el que los problemas pueden describirse. El caso del agua en la tierra, por ejemplo, podría escribirse como un valor W (conteo observado de agua) que se distribuye en una biomial(N,p), con N el número total de pruebas y p con un prior uniforme entre 0 y 1. Una vez que conocemos el modelo de esta manera, conocemos automáticamente todos sus supuestos, ya que sabemos que la distribución binomial supone que cada muestra (lanzamiento de globo) es independiente de la otra, y también sabemos que el modelo supone que los puntos de muestra son independientes entre sí.

### 4.3. Un modelo gaussiano de altura

* A partir de ahora vamos a construir una regresión lineal. En realidad será regresión cuando tengamos un predictor en él. Por lo que para empezar, plantearemos la variable respuesta. Hay un número infinito de posibles distribuciones gaussianas: algunas tienen medias más chicas o más grandes, varianzas más chicas o más grandes. Para construir un modelo, queremos que nuestra máquina bayesiana considere todas las distribuciones posibles, y clasificarlas por plausibilidad posterior, que es la compatibilidad lógica de cada posible distribución con los datos y el modelo.
```r
library(rethinking) 4.7
data(Howell1) # 544 individuos con su altura (en centímetros), su peso (en kilos), su edad y si son hombres o mujeres.
d <- Howell1
precis( d )
'data.frame': 544 obs. of 4 variables:
mean sd 5.5% 94.5% histogram
height 138.26 27.60 81.11 165.74 ▁▁▁▁▁▁▁▂▁▇▇▅▁
weight 35.61 14.72 9.36 54.50 ▁▂▃▂▂▂▂▅▇▇▃▂▁
age 29.34 20.75 1.00 66.13 ▇▅▅▃▅▂▂▁▁
male 0.47 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇

d2 <- d[d$age>=18,] # Filtro adultos
```
* Si nos concentramos únicamente en la altura, vemos que la densidad tiene una forma similar a la normal. Como se trata de una normal, podría ser una acumulación de distribuciones subyacentes, como ya vimos. Pero usemos la distribución normal: ¿cuál hay que usar? necestamos una combinación media-varianza, que no tenemos aún. Por eso escribimos hi~Normal(μ; σ), asumiendo iid: asumiendo que todos los valores tienen igual probabilidad. Es una suposición que muchas veces no parece probable en el mundo físico, pero sí en dentro del modelo.
* :gem: La suposición iid es acerca de cómo el golem representa su incertidumbre. Es un supuesto epistemológico. No es una suposición física sobre el mundo, uno ontológico, a menos que insistas en que lo es. E. T. Jaynes (1922-1998) llamó a esto la falacia de proyección, el error de confundir afirmaciones epistemológicas con afirmaciones ontológicas.
* Para completar el modelo es necesario generar los priors de nuestros parámetros. μ tendrá una distribución normal(178,20) y σ una uniforme(0,50), definidas en forma arbitraria por ideas previas sobre la forma en la que esas variables pueden moverse: 178 parece una altura promedio y un desvío de 20 parece lógico: la desviación estándar es desconocida. Con esos datos ya es posible usar μ y σ como bolsas de dónde extraer datos, y generar una simulación predictiva del prior. Es decir, generar 1000 observaciones del prior y ver cómo se distribuye.
```r
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )
```
* Esto permite validar los priors. Una de las personas más altas de la historia registrada, Robert Pershing Wadlow (1918-1940) medía 272 cm de altura. En nuestra simulación predictiva anterior, el 18% de las personas son más altos que esto. ¿Importa esto? Si tenemos muchos datos, esto puede resultar no importante e inofensivo. Pero ese no siempre será el caso. Hay muchos problemas de inferencia
para los que los datos por sí solos no son suficientes, por numerosos que sean. Bayes nos deja proceder en estos casos. Pero solo si usamos nuestro conocimiento científico para construir priors sensatos.
* Llegó el momento de computar el posterior a partir de una aproximación cuadrática.
```r
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]
flist <- alist(
height ~ dnorm( mu , sigma ) ,
mu ~ dnorm( 178 , 20 ) ,
sigma ~ dunif( 0 , 50 )
)
m4.1 <- quap( flist , data=d2 )
precis( m4.1 )

    Mean StdDev 5.5% 94.5%
mu 154.61 0.41 153.95 155.27
sigma 7.73 0.29 7.27 8.20
```
* Estos números proporcionan aproximaciones gaussianas para la distribución marginal de cada parámetro. Esto significa la plausibilidad de cada valor de µ, después de promediar las plausibilidades de cada valor de σ, viene dado por una distribución gaussiana con media 154,6 y desviación estándar 0,4. Es posible obtener visualizaciones del posterior

```r
library(rethinking)
post <- extract.samples( m4.1 , n=1e4 )
head(post)
mu sigma
1 155.0031 7.443893
2 154.0347 7.771255
3 154.9157 7.822178
4 154.4252 7.530331
5 154.5307 7.655490

precis(post)
quap posterior: 10000 samples from m4.1
mean sd 5.5% 94.5% histogram
mu 154.61 0.41 153.95 155.27 ▁▁▁▅▇▂▁▁
sigma 7.72 0.29 7.26 8.18 ▁▁▁▂▅▇▇▃▁▁▁▁
```

### 4.4. Agregando un predictor

* Hasta ahora lo que hicimos no fue estrictamente una regresión, ya que simplemente generamos un modelo gaussiano para la altura en una población de adultos. La regresión obviamente no es eso, sino que tiene que ver con la información de las variables. El concepto de 'regresión a la media' lo acuñó Galton, al observar una contracción a medida que cada medición informa a las demás. Predecir la altura de un hijo teniendo la de su padre es un problema: predecir la relación entre la altura de los padres con la de los hijos teniendo muchos es más sencillo.
* La estrategia, llamada modelo lineal, es **instruir al golem para que asuma que la variable predictora tiene una relación constante y aditiva con la media del resultado**. El golem luego calcula la distribución posterior de esta relación constante. μi = α + βxi (aquí está la linealidad, que podría reemplazarse por ejemplo por μi = α * exp(-βxi), no alterando los resultados en la práctica). La distribución del posterior rankea las infinitas combinaciones posibles de parámetros por su plausibilidad. μ depende linealmente de dos parámetros, por lo que está implícitamente paramaetrizado: deja de ser un parámetro a estimar. El nuevo parámetro β tendrá su prior, también. No hay nada de especial respecto de la relación lineal, que también podría ser de otra forma (por ejemplo exponencial).
    * *“Golem: Considera todas las líneas que relacionan una variable con la otra. Clasifica todas estas líneas por plausibilidad, dados estos datos.” El golem responde con una distribución posterior.*
* Para aproximar el posterior, modificamos el código que hemos visto, incorporando un nuevo parámetro β.
```r
m4.3 <- quap(
alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*weight ,
    a ~ dnorm( 156 , 100 ) ,
    b ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) ,
data=d2 )

Mean StdDev 5.5% 94.5% a b sigma
a 154.60 0.27 154.17 155.03 1 0 0
b 0.91 0.04 0.84 0.97 0 1 0
sigma 5.07 0.19 4.77 5.38 0 0 1
```

* El resultado es el conocido: la estimación de los parámetros con su media, su desviación estándar y su 5% y 95%. Con estos resultados podría graficarse la línea, con los intervalos en los que podría estar. También podría condicionarse por un valor del predictor, graficando la campana de valores del posterior que se genera.
golems no pueden hacer por sí mismos.
* Más allá de la estimación del parámetro que nos interesa, podríamos generar intervalos de confianza para los valores reales de las observaciones, que gráficamente serían bandas en torno a la media, más amplias que las del propio desvío estandar. Estas bandas incoporan la incertidumbre respecto de las predictoras, que hemos descripto en la primera linea del alist().
* Ajustar un modelo lineal, como se dijo, no es la única forma de proceder en estos casos. Aun con un solo predictor, podría ajustarse un modelo polinómico, que tiene más de un término: la misma variable en forma lineal y cuadrática, por ejemplo, o en forma lineal, cuadrática y cúbica. La palabra 'lineal' significa diferentes cosas en diferentes contextos, y diferentes personas lo usan de manera diferente en el mismo contexto. Lo que significa “lineal” en este contexto es que µi es una función lineal de cualquier parámetro individual. Tales modelos tienen la ventaja de ser más fáciles de ajustar a los datos. También suelen ser más fáciles de interpretar, porque suponga que los parámetros actúan independientemente de la media. Tienen la desventaja de ser fuertemente convencional. A menudo se utilizan sin pensar. Cuando tenga un conocimiento real de su sistema de estudio, a menudo es fácil hacerlo mejor que un modelo lineal. Estos modelos son motores geocéntricos, dispositivos para
describir correlaciones parciales entre variables. El procedimiento para un modelo polinómico en R sería:

```r
d$weight.s <- ( d$weight - mean(d$weight) )/sd(d$weight) # Estandarizo la variable antes de ajustar 
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b1*weight.s + b2*weight.s2 ,
a ~ dnorm( 178 , 100 ) ,
b1 ~ dnorm( 0 , 10 ) ,
b2 ~ dnorm( 0 , 10 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d )

Mean StdDev 5.5% 94.5%
a 146.66 0.37 146.07 147.26
b1 21.40 0.29 20.94 21.86
b2 -8.42 0.28 -8.87 -7.97
sigma 5.75 0.17 5.47 6.03
```
* Otra forma de introducir una curva es construir algo conocido como splines on anclas. La palabra spline originalmente se refería a una pieza larga y delgada de madera o metal que podría anclarse en algunos lugares para ayudar a los dibujantes o diseñadores a dibujar curvas. En estadísticas, una spline es una función suave construida a partir de funciones de componentes más pequeñas. Existen en realidad muchos tipos de splines. Las b-spline que veremos aquí son las más comunes y tienen muchas propiedades interesantes. La "B" significa "base", que aquí solo significa "componente". Las Bsplines crean funciones onduladas a partir de componentes más simples y menos ondulados. Esos componentes se llaman funciones base. La breve explicación de B-splines es que dividen el rango completo de algún predictor variable, como el año, en partes. Luego asignan un parámetro a cada parte. Estos parámetros se encienden y apagan gradualmente de una manera que convierte su suma en una elegante curva ondulada.
* :gem: ¿Qué significan los parámetros? Un problema básico con la interpretación de estimaciones basadas en modelos es conocer el significado de los parámetros. Sin embargo, no hay consenso sobre lo que significa un parámetro porque diferentes personas adoptan diferentes posturas filosóficas hacia los modelos, la probabilidad y predicción. La perspectiva en este libro es una perspectiva bayesiana común: probabilidades posteriores de los valores de los parámetros describen la compatibilidad relativa de los diferentes estados del mundo con los datos, según el modelo. Estos son números de mundo pequeño (Capítulo 2). Así que la gente razonable puede estar en desacuerdo sobre el significado del gran mundo, y los detalles de esos desacuerdos dependen en gran medida del contexto. Tales desacuerdos son productivos, porque conducen a la crítica y revisión del modelo, algo que


### 4.5. Resumen

* Este capítulo introdujo el modelo de regresión lineal simple, un marco para estimar la asociación entre una variable predictiva y una variable de resultado. La distribución gaussiana comprende la probabilidad en tales modelos, porque cuenta los números relativos de maneras diferentes combinaciones de medias y desviaciones estándar pueden producir una observación. El siguiente capítulo amplía estos conceptos al introducir modelos de regresión con más de una variable predictiva.
