## 2. MUNDOS CHICOS Y MUNDOS GRANDES: *teoría de la probabilidad*

* :gem: _A diferencia de Cristobal Colón que pensó que el mundo, de acuerdo a sus conocimientos, era mucho más chico de lo que realmente era,_ el desafío estadístico principal es tratar de que la navegación entre el mundo de lo que se conoce al realizar el modelo al mundo de lo que aún no se conoce, sea lo más tranquila posible. 
* Asumiendo que el mundo pequeño es una descripción precisa del mundo real, los modelos bayesianos son los mejores. Sin embargo, la efectividad del modelo es jústamente su capacidad de predicción en el mundo grande, el exterior: a diferencia del pequeño, la efectividad en el mundo grande tiene que demostrarse en lugar de deducirse lógicamente. 
* Este capítulo, que explica la teoría de la probabilidad en su esencia, se centra en el mundo pequeño. Es la base para el próximo en el que se verá el mundo grande.

### 2.1. El jardín de los datos que se bifurcan

* La inferencia bayesiana es realmente solo contar y comparar posibilidades. :gem: _Como en el cuento de Borges, para hacer una buena inferencia sobre lo que realmente sucedió, es útil considerar todo lo que podría haber sucedido._ Veamos los elementos centrales del enfoque bayesiano:
1. El método de 'contar posibilidades' es, básicamente, dada una posibilidad observada (con reposición, de cinco bolas blancas y azules observamos ABA), cuantas chances hay de cada opción (AAAA tiene 0 chances, BBBB también, ABBB, AABB, AAAB tienen respectivamente tres (3 * 1 * 1), ocho (2 * 2 * 2) y nueve (3 * 1 * 3)). Este método bayesiano es el más fácil de explicar, pero durante un tiempo se lo criticó por dificil de aplicar.
2. La actualización de la probabilidad, implica que por ejemplo **si se obtiene una nueva observación A, cada una se multiplica** y las nuevas probabilidades serán 3, 16 y 27. Este enfoque actualiza la probabilidad con la información, cosa que en este caso es de la misma naturaleza (una nueva observación) pero podría ser de otra, como información sobre la bolsa. Esta idea de actualización a partir de la información, usada a la inversa, habla de una 'ignorancia original' en las probabilidades, que hace que muchas veces en ausencia de información se considere que todo es igualmente probable.
* La idea transversal de la ignorancia bayesiana es clara: **cuando no sabemos qué causó lo que vemos, las opciones que podrían producir lo que vemos de más formas posibles son las más plausibles**. La posibilidad no se mide en chances absolutas sino en relativas al total, es decir, en forma de probabilidad.

### 2.2. Construyendo un modelo

* La inferencia bayesiana, entonces, trabaja con probabilidades en lugar de recuentos sin procesar, por lo que se hace más fácil pero se ve más dificil. Pensemos en el ejemplo: una muestra de parcelas del planeta, buscando encontrar la proporción tierra/agua, da 6 parcelas de agua y 3 de tierra. Diseñar un modelo bayesiano simple es un proceso compuesto por tres pasos:
1. El análisis de datos bayesianos generalmente necesita producir una historia sobre cómo los datos llegaron a ser. Puede ser descriptiva, especificando asociaciones que pueden usarse para predecir resultados, dadas observaciones. O puede ser causal, una teoría de cómo algunos eventos producen otros eventos. En este caso, la historia es que existe una proporción real p de agua en el planeta, que cada parcela aleatoria tiene una probabilidad p de ser agua y 1-p de ser tierra, y que cada parcela es independiente de la otra.
2. El segundo paso es la actualización de esas probabilidades p desconocidas. Programemos nuestra máquina bayesiana para asignar inicialmente la misma plausibilidad para cada proporción de agua, cada valor de p. La ignorancia total es de una p igual a 1-p, o sea 0.5, y cada parcela nueva va actualizando las probabilidades de la siguiente manera:
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/2.1.png"> </p>
Cada conjunto actualizado de plausibilidades se convierte en las plausibilidades iniciales para la siguiente observación. Cada conclusión es el punto de partida para futuras inferencias, pero podría presentarse al revés: de cada probabilidad post actualización podría derivarse la anterior. La actualización permanente es lo que diferencia al bayesiano del no-bayesiano: mientras que el bayesiano tiene que elegir una probabilidad inicial (e ir actualizandola cada vez) el no bayesiano tiene que elegir un estimador. Por eso el bayesiano no necesita un mínimo de observaciones para devolver un resultado.
3. La máquina bayesiana garantiza una inferencia perfecta dentro del pequeño mundo. Esto no tendría utilidad, entonces, si hay diferencias importantes entre el modelo y la realidad. Incluso si los dos mundos coincidieran, cualquier muestra particular de datos podría ser engañosa. La etapa de validar el modelo suele prestar a confusiones, ya que muchas veces se cae en el riesgo de pensar que el modelo debería ser perfecto como máquina, y no que debe cumplir su función de ser útil en un contexto particular. No existe un enfoque de inferencia que brinde garantías universales. Ninguna rama de las matemáticas tiene acceso ilimitado a la realidad, porque las matemáticas no se descubren, sino que se inventan.

### 2.3. Componentes de un modelo

* Llega el momento de pensar en los componentes de un modelo estadístico. Siguiendo lo hablado, lo indispensable es: el número de formas en que cada conjetura podría producir una observación, el número acumulado de formas en que cada conjetura podría producir los datos completos, la plausibilidad inicial de cada causa conjeturada de los datos. Se trata de la función de verosimilitud, los parámetros y el prior.
* La **función de verosimilitud** es el elemento más importante del análisis bayesiano, una fórmula matemática que especifica la plausibilidad de los datos. En el caso del agua y la tierra, esto se traduce en una distribución binomial: el resultado solo puede ser de dos tipos. El recuento de observaciones de "agua" w se distribuye binomialmente, con probabilidad p de "agua" en cada lanzamiento y n lanzamientos en total. ¿Cuál es la probabilidad de encontrar 6 agua en 9, si p=0.5?
``` r
dbinom(6, size=9, prob=0.5) 
0.16
```
* Los **parámetros** son inputs ajustables, como en el caso de la tierra la probabilidad p, el tamaño de la muestra n y el número de 'aguas' observadas, w. Los últimos dos se toman como fijos porque fueron observados, pero podrían no serlos. En este caso, el modelo trabaja para descubrir p. Las preguntas que hacemos sobre los datos a veces se responden directamente mediante parámetros: ¿Cuál es la diferencia promedio entre los grupos de tratamiento? ¿Qué tan fuerte es la asociación entre un tratamiento y un resultado? ¿El efecto del tratamiento depende de una covariable? ¿Cuánta variación hay entre los grupos? Justamente en el mundo bayesiano es donde los parámetros y los datos a veces pueden confundirse.
* Los parámetros son inputs ajustables, como en el caso de la tierra la probabilidad p, el tamaño de la muestra n y el número de 'aguas' observadas, w. Los últimos dos se toman como fijos porque fueron observados, pero podrían no serlos. En este caso, el modelo trabaja para descubrir p. Las preguntas que hacemos sobre los datos a veces se responden directamente mediante parámetros: ¿Cuál es la diferencia promedio entre los grupos de tratamiento? ¿Qué tan fuerte es la asociación entre un tratamiento y un resultado? ¿El efecto del tratamiento depende de una covariable? ¿Cuánta variación hay entre los grupos? Justamente en el mundo bayesiano es donde los parámetros y los datos a veces pueden confundirse.
* Para cada parámetro que desee que calcule su máquina bayesiana, debe proporcionar a la máquina un prior. Una máquina bayesiana debe tener una asignación de plausibilidad inicial para cada posible valor del parámetro. Los priors son distribuciones de probabilidad del parámetro, y enfatizan el caracter 'subjetivo' de estos modelos.
* Un modelo bayesiano trata las estimaciones como una consecuencia puramente lógica de esos supuestos. Para cada combinación única de datos, probabilidad, parámetros, y prior, hay un conjunto único de estimaciones. La plausibilidad relativa de los diferentes valores de parámetros condicionales a los datos se conocen como distribución posterior. La fórmula del posterior surge de la idea de Bayes, de que ya que ```Pr(w,p) = Pr(w|p) * Pr(p)``` y que ```Pr(w,p) = Pr(p|w) * Pr(w)``` entonces sucederá que ```Pr(p|w) = Pr(w|p) * Pr(p) / Pr(w)```, que implica ```Posterior = Verosimilitud * Prior / Verosimilitud``` en promedio. Este último término se ocupa de 'estandarizar' el posterior, para que sea una probabilidad y siempre esté entre 0 y 1.
* Ejemplo: Buscamos conocer que proporción de los estudiantes siguen esta recomendación de 2500Kcal o más, para ello tomaremos una muestra aleatoria de estudiantes. ![$\theta$](https://render.githubusercontent.com/render/math?math=%24%5Ctheta%24)
es la proporción que ingieren en un día 2500 kcal o más, que es desconocido, y desde el punto de vista bayesiano cuando tenemos incertidumbre de algo (puede ser un parámetro o una predicción) lo vemos como una variable aleatoria con una distribución de probabilidad que actualizaremos conforme obtenemos información (observamos datos). La distribución ![$p(\theta)$](https://render.githubusercontent.com/render/math?math=%24p(%5Ctheta)%24)
 se conoce como la distribución a priori y representa nuestras creencias de los posibles valores que puede tomar el parámetro. Podría ser, por ejemplo
```r
theta <- c(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85 0.95)
pesos.prior <- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)
```
Esa distribución prior se multiplica por la verosimilitud, que en este caso sería bernoulli. Si uno observa 30 estudiantes donde 11 cumplen la condición, la fórmula es ![$\mathcal{L}(\theta) = \theta^11(1-\theta)^19$](https://render.githubusercontent.com/render/math?math=%24%5Cmathcal%7BL%7D(%5Ctheta)%20%3D%20%5Ctheta%5E11(1-%5Ctheta)%5E19%24), y la actualización devuelve un resultado así:
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/2-2.png"> </p>.
Se llegó a lo que se buscaba, que es la distribución de probabilidad de consumir las calorías adecuadas.
* La lección clave es que **el posterior es proporcional al producto del prior y la probabilidad**. ¿Por qué? Debido a que el número de caminos en el jardín es el producto de la cantidad anterior de rutas y la nueva cantidad de rutas. La probabilidad indica el número de caminos, y el prior indica el número prior. La multiplicación es solo recuento comprimido. La probabilidad promedio en la parte inferior simplemente estandariza los recuentos, para que sumen uno. Una aclaración importante es que el uso del teorema de bayes no es exclusivo del análisis bayesiano: lo que sí es exclusivo es su uso para cuantificar la incertidumbre sobre entidades teóricas que no se pueden observar, como parámetros y modelos.

### 2.4. Haciendo al modelo andar

* Se puede pensar la acción del modelo como condicionar el priar sobre los datos. Se necesitan varias técnicas numéricas para aproximar las matemáticas que se derivan de la definición del teorema de Bayes. En este libro se verán tres:
1. Si bien la mayoría de los parámetros son continuos, capaces de asumir un número infinito de valores, resulta que podemos lograr una excelente aproximación de la distribución posterior considerando solo una cuadrícula finita de valores de parámetros. El procedimiento se llama aproximación de cuadrícula y consta de definir una cuadrícula, y actualizar pensando en prior * probability. Puede replicarse de la siguiente forma:
```r
# Definir cuadrícula
p_grid <- seq( from=0 , to=1 , length.out=20 )
# Definir priors
prior <- rep( 1 , 20 ) # otros ejemplos de prior: prior <- ifelse( p_grid < 0.5 , 0 , 1 ); prior <- exp( -5*abs( p_grid - 0.5 ) )
# Definir probabilidad de cada valor en la cuadrícula
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
# Calcular posterior
unstd.posterior <- likelihood * prior
# Estandarizar el posterior
posterior <- unstd.posterior / sum(unstd.posterior)

plot(p_grid,posterior,type="b",xlab="probability of water",ylab="posterior probability")
```
2. La estrategia de aproximación de cuadrícula se escala muy mal con la complejidad del modelo, ya que en estos días son comunes los modelos con cientos o miles de parámetros, y con la cuadrícula escalan exponencialmente. Un enfoque útil es la aproximación cuadrática. En condiciones bastante generales, la región cerca del pico de la distribución posterior será casi gaussiana o "normal". Una aproximación gaussiana se llama "aproximación cuadrática" porque el logaritmo de una distribución gaussiana forma una parábola. Y una parábola es una función cuadrática. La aproximación cuadrática representa esencialmente cualquier log-posterior con una parábola. Lo que se necesita, entonces, es encontrar el 'pico' del posterior y luego estimar la curvatura cerca de él.
```r
library(rethinking)
globe.qa <- map(
alist(
w ~ dbinom(9,p) , # binomial likelihood
p ~ dunif(0,1) # uniform prior
) ,
data=list(w=6) )
# display summary of quadratic approximation
precis(globe.qa)

Mean StdDev 5.5% 94.5%
p 0.67 0.16 0.42 0.92
```
Al igual que en el caso anterior, la precisión aumenta cuando la cantidad de datos también lo hace. La aproximación cuadrática generalmente es equivalente a una estimación de máxima verosimilitud (MLE).
3. En ocasiones, la distribución del posterior no puede calcularse en forma única sino que debe hacerlo de a partes, lo cual quita poder a la aproximación cuadrática. Como resultado surgieron modelos más complejos como el de las cadenas de Markov Monte Carlo (MCMC), que en lugar de intentar aproximar la distribución posterior directamente, toma muestras de la parte posterior y a partir de esa colección de valores de parámetros y frecuencias de esos valores en el posterior, realizan un histograma y construyen la distribución del posterior.

### 2.5. Resumen
* El objetivo de la inferencia bayesiana es la distribución de probabilidad posterior, que es el número relativo de formas en que cada causa conjeturada de los datos podría haber producido los datos, valor que se actualiza cuando más datos se conocen.
* Más mecánicamente, un modelo bayesiano es una combinación de probabilidad, una elección de parámetros, y un prior. La probabilidad proporciona la plausibilidad de una observación (datos), dado un valor fijo para los parámetros. El prior proporciona la plausibilidad de cada valor posible de los parámetros, antes de contabilizar los datos.
* En la práctica, los modelos bayesianos se ajustan a los datos mediante técnicas numéricas, como la aproximación a la cuadrícula, la
aproximación cuadrática, y las cadenas de Markov de Monte Carlo. Cada método tiene distintos trade-offs.
