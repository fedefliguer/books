## 3. MUESTREANDO LO IMAGINARIO: *introducción a las muestras*

* El uso contraintuitivo del teorema de bayes para inferencia del posterior es conocido, cuando se trata de grupos muy desbalanceados
``` r
PrPV <- 0.95 # Un test detecta 95% de las veces a los vampiros analizando la sangre, es bastante preciso. P(Test positivo|Vampiro) = 0.95.
PrPM <- 0.01 # Hay un 1% de las veces en las que dice que los mortales son vampiros, errando el diagnóstico. P(Test positivo|Mortal) = 0.01 
PrV <- 0.001 # La muestra está muy desbalanceada: hay muy pocos vampiros P(Vampiro) = 0.001.

( PrVP <- PrPV*PrV / PrP ) # Teorema de bayes
0.08683729
```
* Entonces, la situación que uno puede observar es la de recibir un test positivo y tener que definir si es o no es un vampiro: P(Vampiro|Test positivo). El hecho de que dado que es un vampiro los tests los identifiquen correctamente con frecuencia, no tiene gran importancia porque esa situación (ser un vampiro) es muy poco frecuente. A la inversa, ese 1% de falsos positivos parece menor, pero los mortales son una mayoría muy grande. Eso hace que recibir el test positivo no dé una evidencia tan contundente. Muchas veces se usa esto para atacar la inferencia bayesiana, pero recordemos que la inferencia no se caracteriza por usar el teorema, sino por una concepción más amplia de la probabilidad.
* Todas las probabilidades usadas en el ejemplo son frecuencias de eventos y no probabilidades de parámetros, por lo tanto este análisis no es exclusivo de la inferencia bayesiana. Si esta historia se explicara más coloquialmente (de 100.000 personas 100 son vampiros; de los 100 vampiros 95 testean positivo; de los 99.900 no vampiros 995 testean positivo), no sería tan contraintuitiva como por medio del teorema.
* En el capítulo anterior vimos que la distribución del posterior es una distribución de probabilidad, y como tal, podemos imaginar muestras de él. Los eventos muestrados, en este caso, son valores de parámetros algunos de los cuales no tienen una realización efectiva. En este capítulo comenzaremos a **usar muestras para resumir y simular la salida del modelo**.

### 3.1. Muestreo desde una cuadrícula posterior aproximada
* Volviendo al problema de la superficie p de agua del planeta. Buscaremos obtener el posterior, que es la probabilidad de p condicionada a los datos observados.
``` r
p_grid <- seq( from=0 , to=1 , length.out=1000 )  # La grilla total de valores que podría tomar el parámetro
[1] 0.000000000 0.001001001 0.002002002 0.003003003 0.004004004 0.005005005 0.006006006 0.007007007

prior <- rep( 1 , 1000 )  # A priori todos son igualmente plausibles (sería lo mismo repetir otro valor que no sea 1)
[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

likelihood <- dbinom( 6 , size=9 , prob=p_grid )  # La verosimilitud es la distribucion, la densidad de probabilidad dado que en 9 casos observé 6 positivos.
[1] 0.000000e+00 8.425225e-17 5.375951e-15 6.105137e-14 3.419945e-13 1.300676e-12 3.872087e-12

posterior <- likelihood * prior # En este caso coincide con el likelihood, precisamente porque mi prior fue constante en 1. Si hubiera sido constante en otra cosa, no coincidiría ahora pero en el paso siguiente sí. Si hubiera sido variable, allí sí hubiera tenido influencia el prior.

posterior <- posterior / sum(posterior)
```
* El posterior puede pensarse como un balde lleno de valores de parámetros, números como 0.1, 0.7, 0.5, 1, etc. Sacando una muestra, siempre que el cubo esté bien mezclado, las muestras resultantes tendrán las mismas proporciones que la densidad posterior exacta.
``` r
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE ) # Sacar una muestra de otdas la grila p_grid, de tamaño 10.000, con probabilidades igual al posterior y con reemplazo.
plot( samples )
```
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/2.3.png"> </p>

### 3.2. Muestrando para resumir
* Pero desde la gráfica anterior, no hay demasiadas certezas. Como se trata de un caso continuo, necesitamos medidas para resumir esta información. ¿Qué valor del parámetro marca el 5% inferior de la probabilidad posterior? ¿Qué rango de valores de parámetros contiene el 90% de la probabilidad posterior? ¿Qué valor del parámetro tiene la mayor probabilidad posterior?
* Se puede devolver la probabilidad posterior de que el parámetro esté entre dos valores, pero sin duda es más común invertirlo: fijar la masa y devolver los valores que contienen esa probabilidad. Estos intervalos posteriores informan dos valores de parámetros que contienen entre ellos una cantidad específica de probabilidad posterior, una masa de probabilidad. A esto se le denomina intervalo de confianza, también.
* El método de los percentiles, sin embargo, resulta útil con formas de distribución como la anterior, pero hay casos en los que podría no ser así.
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/2.4.png"> </p>

* En este caso el intervalo que contiene el 50% central, entre el percentil 25 y el 75, no contiene a los valores más frecuentes de probabilidad. Surgen medidas como el intervalo mayor de densidad posterior (HPDI), que se muestra a la derecha, que es el intervalo más estrecho que contiene esa probabilidad. En la mayoría de los casos, estas dos medidas son similares. Como regla general, **si la elección del tipo de intervalo marca una gran diferencia, entonces no debería usar intervalos para resumir el posterior.**
* En estricta inferencia no bayesiana, afirmar que hay una probabilidad del X% de que un parámetro esté en un intervalo es incorrecto, porque la inferencia estricta no bayesiana prohíbe usar la probabilidad para medir la incertidumbre sobre parámetros. En cambio, uno debería decir que si repetimos el estudio y el análisis una gran cantidad de veces, entonces el 95% de los intervalos calculados contendrían el verdadero valor del parámetro. La mayoría de los científicos encuentran que la definición de un intervalo de confianza es desconcertante, y muchos de ellos se deslizan inconscientemente en una interpretación bayesiana.
* En inferencia no bayesiana, no es posible usar probabilidades para medir la incerteza sobre parámetros. Por lo tanto, no sería correcto decir que un intervalo de 95% de confianza significa que hay una probabilidad de 95% de que el verdadero parámetro esté en el intervalo. Más bien, que si repetimos el estudio una larga cantidad de veces, 95% de los intervalos computados tendrían el verdadero valor del parámetro. Más allá de la interpretación bayesiana o no, no es correcto decir que el intervalo de 95% contiene el valor 95% del tiempo. El 95% funciona en el mundo pequeño, por lo que no aplicará exactamente al mundo grande o el mundo real.
* Si lo que queremos es devolver un único punto, una respuesta sobre el parámetro que se nos pide, surge otra cuestión: dada toda la distribución posterior, ¿qué valor deberías informar? En general esto no es necesario, pero podría buscarse el máximo (la moda), la mediana, la media. Si se quiere decidir una, podría usarse el criterio de la función de pérdida: si la pérdida es la diferencia en valor absoluto entre el valor real del parámetro y el valor estimado, entonces el óptimo es la mediana. Si la pérdida en cambio es cuadrática, el valor óptimo es la media.

### 3.3. Muestrando para simular predicciones
* Una vez que el modelo se ajustó a los datos reales, es frecuente usarlo para simular observaciones implícitas. Esto es útil para validar el modelo, para validar el software y también con propósitos de investigación, para saber más sobre el diseño de los experimentos.
* En el ejemplo del agua y la tierra, simular datos sería en forma simulada. Un experimento controlado sería con n=2, buscando dos puntos en el planeta. Hay tres posibles observaciones: 0 agua, 1 agua, 2 agua. Si la probabilidad de agua es 0.7, entonces las probabilidaes son estas:
``` r
dbinom( 0:2 , size=2 , prob=0.7 )
0.09 0.42 0.49 # Respectivamente, 0 agua, 1 agua, 2 agua
rbinom( 10 , size=2 , prob=0.7 )
2 2 2 1 2 1 1 1 0 2 # Cada caso es un dato simulado, que devuelve 0, 1 o 2.
dummy_w <- rbinom( 1e5 , size=9 , prob=0.7 ) # Más simulaciones, y n=9 es el experimento que hemos hecho 
simplehist( dummy_w , xlab="dummy water count" ) # La respuesta se parece a lo que vimos antes
```
1. ¿El software funcionó? En el caso más simple, podemos verificar si el software funcionó comprobando la correspondencia entre las predicciones implícitas y los datos utilizados se ajusta al modelo. A esas predicciones implícitas se las denominan 'retrodicciones', y no es que si aproximadamente coinciden entonces el software funcionó, pero si son muy distintas posiblemente haya hecho algo mal.
2. ¿Es adecuado el modelo? El objetivo no es probar si los supuestos del modelo son "verdaderos", porque todos los modelos son falsos. Más bien, el objetivo es evaluar exactamente cómo el modelo no puede describir los datos, como un camino hacia la comprensión del modelo, revisión y mejora. Todos los modelos fallan en algún aspecto, por lo que debe usar su juicio, así como los juicios de sus colegas, para decidir si cualquier falla en particular es o no importante. Las predicciones implícitas son inciertas en dos sentidos: hay incertidumbre sobre las observaciones, e incertidumbre sobre el parámetro. **Para cada valor posible del parámetro p, hay una distribución implícita**. Entonces, si tuviera que calcular la distribución muestral de resultados en cada valor de p, entonces podría promediar todas estas distribuciones de predicción juntas, usando probabilidades posteriores de cada valor de p, para obtener una distribución predictiva posterior.
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/3.3.png"> </p>

* Las predicciones del modelo simulado son bastante consistentes con los datos observados en este caso: el recuento real de 6 se encuentra justo en el medio de la distribución simulada. Ahi esta bastante propagación a las predicciones, pero gran parte de esta propagación surge del proceso binomial en sí mismo, no de la incertidumbre sobre p.

### 3.4. Resumen
* Este capítulo introdujo los procedimientos básicos para manipular distribuciones posteriores. Nuestra herramienta fundamental son muestras de valores de parámetros extraídos de la distribución posterior. Trabajar con muestras transforma un problema de cálculo integral en un problema de resumen de datos. Estas muestras se pueden usar para producir intervalos, estimaciones puntuales, etc.
* Las comprobaciones predictivas posteriores combinan incertidumbre sobre los parámetros, tal como lo describe la distribución posterior, con incertidumbre sobre los resultados, según lo descrito por la función de probabilidad supuesta. Estas comprobaciones son útiles para verificar que su software funcionó correctamente, pero también para para buscar formas en que sus modelos son inadecuados.
