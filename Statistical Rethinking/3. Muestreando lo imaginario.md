## 2. MUESTREANDO LO IMAGINARIO: *introducción a las muestras*

* El uso contraintuitivo del teorema de bayes para inferencia del posterior es conocido, cuando se trata de grupos muy desbalanceados
``` r
PrPV <- 0.95 # Un test detecta 95% de las veces a los vampiros analizando la sangre
PrPM <- 0.01 # Hay un 1% de las veces en las que dice que los mortales son vampiros, errando el diagnóstico
PrV <- 0.001 # La muestra está muy desbalanceada: hay muy pocos vampiros
PrP <- PrPV*PrV + PrPM*(1-PrV)
( PrVP <- PrPV*PrV / PrP ) # Teorema de bayes
0.08683729
```
* Lo que ocurre es que los resultados más positivos son falsos positivos, incluso cuando los verdaderos positivos son detectados correctamente. Muchas veces se usa esto para atacar la inferencia bayesiana, pero recordemos que la inferencia no se caracteriza por usar el teorema, sino por una concepción más amplia de la probabilidad. Todas las probabilidades usadas en el ejemplo son frecuencias de eventos y no probabilidades de parámetros, por lo tanto este análisis no es exclusivo de la inferencia bayesiana. Si esta historia se explicara más coloquialmente, no sería tan contraintuitiva como por medio del teorema.
* En el capítulo anterior vimos que la distribución del posterior es una distribución de probabilidad, y como tal, podemos imaginar muestras de él. Los eventos muestrados, en este caso, son valores de parámetros algunos de los cuales no tienen una realización efectiva. En este capítulo comenzaremos a usar muestras para resumir y simular la salida del modelo.

### 3.1. Muestreo desde una cuadrícula posterior aproximada
``` r
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep( 1 , 1000 )
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
```
* El posterior puede pensarse como un balde lleno de valores de parámetros, números como 0.1, 0.7, 0.5, 1, etc. Sacando una muestra, siempre que el cubo esté bien mezclado, las muestras resultantes tendrán las mismas proporciones que la densidad posterior exacta.
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/2.3.png"> </p>

### 3.2. Muestrando para resumir
* Pero desde la gráfica anterior, no hay demasiadas certezas. Como se trata de un caso continuo, necesitamos medidas para resumir esta información. ¿Qué valor del parámetro marca el 5% inferior de la probabilidad posterior? ¿Qué rango de valores de parámetros contiene el 90% de la probabilidad posterior? ¿Qué valor del parámetro tiene la mayor probabilidad posterior?
* Se puede devolver la probabilidad posterior de que el parámetro esté entre dos valores, pero sin duda es más común invertirlo: fijar la masa y devolver los valores que contienen esa probabilidad. Estos intervalos posteriores informan dos valores de parámetros que contienen entre ellos una cantidad específica de probabilidad posterior, una masa de probabilidad. A esto se le denomina intervalo de confianza, también.
* El método de los percentiles, sin embargo, resulta útil con formas de distribución como la anterior, pero hay casos en los que podría no ser así.
<p align="center"> <img src="https://github.com/fedefliguer/books/blob/master/SR-images/2.4.png"> </p>
* En este caso el intervalo que contiene el 50% central, entre el percentil 25 y el 75, no contiene a los valores más frecuentes de probabilidad. Surgen medidas como el intervalo mayor de densidad posterior (HPDI), que se muestra a la derecha, que es el intervalo más estrecho que contiene esa probabilidad.
