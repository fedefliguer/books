## 7. EL COMPÁS DE ULISES

* Nicolás Copernico abogó por reemplazar el modeo geocéntrico por uno heliocéntrico, que era más 'armonioso'. Esta historia se ha convertido en una fábula del triunfo de la ciencia sobre la ideología y la superstición. El modelo de Copernico hacía las mismas predicciones que el geocéntrico, pero era más complejo. Dado que en precisión eran equivalentes, había razones para preferir el más simple. En este capítulo se verán los casos en los que la precisión y la simplicidad se compensan, basado en los tres monstruos: el Underfitting, el Overfitting y el Confounding. Nuestro trabajo será ver métodos para no caer en esto: regularización para que el modelo no se emocione demasiado con los datos, o bien métodos como Information Criteria o Cross Validation.

### 7.1. El problema de los parámetros

* Lo que hemos visto hasta ahora es que agregar variables puede ayudar a revelar los efectos causales reales. Hemos visto que agregar variables puede ayudar a predecir mejor, a costa de esconder influencias reales causales. Entonces, si no queremos entender la asociación causal sino predecir, siempre está bien agregar variables?
* El sobreajuste ocurre cuando un modelo aprende demasiado de la muestra. Lo que esto significa es que hay tanto características regulares como irregulares. Las características regulares son los objetivos de nuestro aprendizaje, porque generalizan bien o responden a una pregunta de interés. Las características irregulares son, en cambio, aspectos de los datos que no se generalizan y por lo que puede engañarnos. En el extremo, si tenemos suficientes parámetros para asignar a cada punto de los datos, la asignación será perfecta y el R2 será 1.
* Los modelos polinómicos sobreajustados logran ajustar los datos extremadamente bien, pero sufren por esta precisión dentro de la muestra al hacer predicciones sin sentido fuera de la muestra. Por el contrario, el ajuste insuficiente produce modelos que son inexactos tanto dentro y fuera de la muestra. Han aprendido muy poco, sin poder recuperar las características regulares de la muestra. Otra forma de analizar si los modelos están sobre o subajustados tiene que ver con el cambio en la curva cuando se quitan observaciones. Si el cambio es extremadamente bajo entonces el modelo podría estar subajustado, mientras que si es extremandamente alto podría estar sobreajustado.

### 7.2. Entropía y accuracy

* Entonces, ¿cómo navegamos entre el overfitting y el underfitting? Por un lado, es posible realizar un análisis de costo-beneficio. ¿Cuánto cuesta cuando nos equivocamos? ¿cuanto hacemos ganar cuando tenemos razón? La mayoría de los científicos nunca hacen estas preguntas de manera formal, pero deberíamos responderlas rutinariamente. Se parte de un ejemplo de dos meteorologos para diez días: uno predice 100% de lluvia tres días en los que llueve y 60% de lluvia los otros siete en los que no llueve. El otro predice 0% de lluvia siempre. Así es que el primero acierta 3 * 100% + 7 * 40% = 5.8, mientras el segundo 3 * 0% + 7 * 100% = 7. Si en cambio hicieramos un scoring que asignara puntajes distintos a cada equivocación, el resultado podría ser otro.
* Si en vez de calcular la accuracy por medio de ese score 'parejo' o de scores ponderados, lo hicieramos por medio de la probabilidad conjunta, deberíamos computar las probabilidades de predecir correctamente la secuencia conjunta, es decir 1 * 1 * 1 * 0.4 * 0.4 * 0.4 * 0.4 * 0.4 * 0.4 * 0.4 = 0.005 para el primer caso y 0 * 0 ...  = 0 para el segundo caso. Esta segunda probabilidad 'cuenta casos' como estamos viendo, desde un enfoque bayesiano. Suele llamarse Log Socring Rule, porque habitualmente busca la log-probabilidad de que la predicción sea perfecta. Se trata de hallar la distancia entre la predicción y el verdadero en caso de fallar, lo cual trae consigo el problema de la determinación de la distancia correcta para usar. La solución al problema de cómo medir la distancia de precisión de un modelo desde un objetivo se proporcionó a fines de la década de 1940, con la Teoría de la Información. La idea básica es preguntar: ¿Cuánto se reduce nuestra incertidumbre al conocer un resultado? Considere las previsiones meteorológicas de nuevo. Los pronósticos se emiten con anticipación y el clima es incierto. Cuando llega el día real, el clima ya no es incierto. La reducción en la incertidumbre es entonces una medida natural de cuánto hemos aprendido, cuánta “información” obtenemos al observar el resultado. Entonces, si podemos desarrollar una definición precisa de "incertidumbre", podemos proporcionar una medida de referencia de lo difícil que es predecir. La disminución medida en la incertidumbre es la definición de información en este contexto.
* Para aplicar esto, necesitamos una forma que nos permita cuantificar la incertidumbre inherente a una distribución de probabilidad. Supongamos que hay dos climas posibles: sol o lluvia. Cada uno de estos eventos ocurre con alguna probabilidad, y estas probabilidades suman uno. Lo que queremos es una función. De ella deseamos tres cosas:
    1. Contínua. Así un cambio chico en las probabilidades no resulte en un cambio grande en la incertidumbre.
    2. Creciente con la mayor cantidad de posibles eventos.
    3. Aditiva. Si tenemos múltiples eventos, las incertidumbres deberían poder sumarse.
* Siguiendo estos tres principios surge la función de entropía de la información, en donde si hay n diferentes posibles eventos y cada evento tiene probabilidad p, la medida de incertidumbre es:
$$H(p) = -Elog(p_{i}) = -\sum_{i=1}^{n}p_{i}log(p_{i})$$
lo que significa que la incertidumbre contenida en una distribución de probabilidad es la probabilidad logarítmica promedio de un evento. Si las verdaderas probabilidades fueran 0.3 de lluvia y 0.7 de sol, entonces $H(p) = 0.61$. Si las verdaderas probabilidades fueran 0.01 de lluvia y 0.99 de sol, entonces $H(p) = 0.06$. Estos valores de incertidumbre no dicen mucho por sí mismos, pero nos pueden ayudar a generar medidas de precisión. En el mismo sentido y cómo se verá luego, maximizar la entropía es buscar la distribución más consistente con los datos. El target tiene una entropía, la variable respuesta en el modelo tiene otra entropía: la diferencia entre ambas es la K-L-Divergence.
* Para calcular esa divergencia debería ser conocido p, la distribución de probabilidad del target. En todos los ejemplos hasta ahora se asumió p conocida, pero si hacemos inferencia estadística es porque no la conocemos. Sin embargo, si lo que buscamos es comparar dos modelos no es necesario conocer esa distribución: si conocemos los scores de log-probabilidades de cada observación de cada modelo (cuya transformación * -2 se denomina Deviance) solo bastará sumarlas para tener la del modelo en sí. Como sucedió antes, se impone el modelo más complejo, a lo que se introduce la cuestión del conjunto de train y el de test.

### 7.3. Regularización

* Una forma de producir mejores predicciones es hacer el modelo peor al ajustar la muestra. Para evitar que un modelo se 'entusiasme' demasiado con la muestra de entrenamiento, se usa un prior "escéptico", uno que ralentiza la tasa de aprendizaje de la muestra. El prior escéptico más común es un prior regularizador. Tal previo, cuando está sintonizado correctamente, reduce el sobreajuste al mismo tiempo que permite que el modelo aprenda las características regulares de un muestra. Sin embargo, si el anterior es demasiado escéptico, se perderán las características regulares.
* Los modelos lineales en los que los parámetros utilizan priores gaussianos centrados en cero se conocen como regresión ridge. λ > 0 da como resultado menos sobreajuste, pero si λ es demasiado grande, corremos el riesgo de sufrir un ajuste insuficiente. Pese a no haber sido desarrollada originalmente como bayesiana, la regresión ridge es otro ejemplo de cómo un procedimiento estadístico puede entenderse desde perspectivas bayesianas y no bayesianas.

### 7.4. Predecir la predictibilidad

* Los dos métodos para evaluar la capacidad predictiva de los modelos son Cross Validation e Information Criteria. 
* Cross-validation es evaluar el modelo en la sobservaciones que quedaron afuera. Las alternativas son k-folds, leave-one-out o Paret-Smoothed Importance Sampling. Este último pondera las observaciones por su 'improbabilidad' y le da más peso a la hora de construir los folds: es muy sensible a outliers, y el libro luego sugiere las regresiones robustas para evitarlas.
* Information Criteria (o Akaike Information Criteria, abreviado AIC) es una soprendentemente simple forma de estimar el promedio del deviance out-of-sample. Lo que dice es que la dimensionalidad del posterior es una medida natural de la tendencia al overfitting. Esto solo funciona bajo ciertos supuestos, y en forma más general el Widely Applicable Information Criterion (WAIC) que es el log-posterior de la función de densidad de las predicciones, más una penalización proporcional a la varianza en las predicciones.

### 7.5. Comparación de modelos

* Una estrategia común con todo lo visto (CV/PSIS/WAIC + Regularización) es la Selección de Modeo, es decir elegir el modelo con el menor valor del criterio elegido. En vez de Selección, hablaremos de Comparación de modelos, que es un enfoque más general que usa múltiples modelos para entender cómo diferentes variables influyen la predicción y, en comparación con un modelo causal, nos ayudan a entender la relación. Se utiliza un ejemplo de lo visto en el capítulo anterior de las plantas y los hongos y las variables de tratamiento, mostrando que la mejor WAIC está en un modelo uqe no asigna correctamente las causas. El libro no llega a abarcar votaciones ni promedios entre modelos.
* :gem: Pensemos en los modelos como caballos de carrera. En cualquier carrera en particular, el mejor caballo puede no ganar. Si el caballo ganador termina la carrera en la mitad del tiempo del segundo, podemos estar bastante seguros de que el caballo ganador también es el mejor. Si en cambio hay un empate cercano entre el primer y el segundo lugar, entonces es mucho más difícil tener confianza en cual es el mejor caballo. Los valores WAIC son análogos a estos tiempos de carrera: los valores más pequeños son mejores, y las distancias entre los caballos/modelos son informativas. Pronosticar futuras carreras/predicción basado en una sola raza/ajuste no conlleva garantías.
* Eventualmente, si intentamos las suficientes combinaciones y transformaciones de variables predictoras, eventualmente podríamos encontrar un modelo que se ajuste
cualquier muestra en particular muy bien. Analogía de overfitting: desde el año 1840 hasta 1960, todos los Presidentes de Estados Unidos elegidos en un año que termina en el dígito 0 han muerto en el cargo.
