_[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)_

## 1. Introducción

* El libro busca hacer que los modelos ML sean fáciles de interpretar. Por lo tanto, la organización de los diferentes modelos que hay será por su capacidad de ser interpretados.
* **ML: conjunto de métodos que usan las computadoras para hacer y mejorar predicciones o comportamientos basados en datos.**
* El libro se ocupa del aprendizaje supervisado. La serie de pasos es la siguiente:
  1. **Recoger** datos pasados con el target.
  2. **Llevar** esos datos a un algoritmo ML.
  3. **Integrar** el resultado de ese algoritmo a un producto o proceso.

## 2. Interpretabilidad

* La **interpretabilidad** es el grado en que un humano puede comprender la causa de una decisión.
* Es posible que algunos modelos no requieran explicaciones porque se usan en un entorno de bajo riesgo, lo que significa que un error no tendrá consecuencias graves (por ejemplo, un sistema de recomendación de películas) o que el método ya ha sido ampliamente estudiado y evaluado (por ejemplo, reconocimiento óptico de caracteres). La necesidad de interpretabilidad surge cuando **no es suficiente obtener la predicción (el qué), sino que el modelo también debe explicar cómo llegó a la predicción (el por qué)**.
* Además de eso, la interpretabilidad es importante en ciertas circunstancias particulares:
  1. En ciertas circunstancias, las personas no se conforman con el qué sino que su curiosidad o su necesidad hace que necesiten saber cómo funcionan ciertas cosas.
  2. En algunos casos, el algoritmo es utilizado por una disciplina científica que busca agregar conocimiento al mundo. En ese caso, no es la predicción lo que interesa sino directamente el algoritmo, es decir que es imprescindible interpretarlo.
  3. Los algoritmos pueden tener sesgos, en particular en contra de ciertas minorías. La aceptabilidad social de un algoritmo depende de su interpretabilidad.
  4. Los modelos solo pueden ser inspeccionados y auditados cuando son interpretables.
* Por el contrario, la interpretabilidad no es necesaria cuando el modelo tiene un riesgo muy bajo al equivocarse, cuando el problema está muy estudiado y hay muchos años de experiencia en el tema, o bien cuando las personas conociendolo podrían modificar su comportamiento para alterar el resultado.
* La **interpretabilidad intrínseca** se refiere a los modelos de aprendizaje automático que se consideran interpretables debido a su estructura simple, como árboles de decisión cortos o modelos lineales dispersos. La **interpretabilidad post hoc** se refiere a la aplicación de métodos de interpretación después del entrenamiento modelo.
* Un método de interpretación puede tener múltiples resultados: estadísticos de resumen, visualización de los atributos, elementos internos del modelo, ejemplos con datos reales. La interpretación también puede ser propia del modelo (como los coeficientes de la regresión) o general (pares de entrada y salida).
* Alcance de la interpretabilidad: Preguntas en las que valdría la pena detenerse.
  1. ¿Cómo crea el algoritmo el modelo? Esto requiere conocimiento del algoritmo, y no sólo de los datos.
  2. ¿Cómo hace predicciones el modelo entrenado? Entender cómo, a través de las variables y los pesos, se pueden generar las predicciones
  3. ¿Cómo afectan partes del modelo a las predicciones? Entender los pesos, a veces son contraintuitivos.
  4. ¿Por qué el modelo hizo una cierta predicción para una instancia?
  5. ¿Por qué el modelo hizo predicciones específicas para un grupo de instancias?

## 3. Modelos
* Un **modelo es lineal** si la asociación entre features y target se modela linealmente.
* Un modelo con restricciones de monotonicidad asegura que la relación entre una característica y el resultado objetivo siempre vaya en la misma dirección en todo el rango de la feature: un aumento en el valor de la característica siempre conduce a un aumento o siempre a una disminución en el objetivo 
* Algunos modelos pueden incluir automáticamente interacciones entre características para predecir el resultado objetivo.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.1.png"> </p>

### 1. Regresión lineal
* En este modelo, el resultado previsto de una instancia es una **suma ponderada de sus características p.**
* Las betas (β) representan los pesos o coeficientes de las características aprendidas. El primer peso en la suma (β0) se llama intercepción y no se multiplica con una característica. El épsilon (ϵ) es el error que cometemos, la diferencia entre la predicción y el resultado real. 
* Se supone que estos errores siguen una distribución gaussiana centrada en torno al 0, lo que significa que **cometemos muchos errores en direcciones negativas y positivas, muchos pequeños y pocos grandes**. Los pesos estimados vienen con intervalos de confianza. Un intervalo de confianza es un rango para la estimación de peso que cubre el peso "verdadero" con una cierta confianza. 
* Si el modelo es el modelo "correcto" depende de si las relaciones en los datos cumplen ciertos supuestos:
  * Linealidad: La predicción debe ser una combinación lineal de características. Los efectos lineales son fáciles de cuantificar y describir. Son aditivos, por lo que es fácil separar los efectos. 
  * Normalidad: Se supone que la esperanza del target condicionada a los features incluidos en la regresión sigue una distribución normal. Si se viola esta suposición, los intervalos de confianza estimados de los pesos de las características no son válidos.
Homocedasticidad (varianza constante): Se supone que la varianza del término de error es constante en todo el espacio de features. Esta suposición a menudo se viola en la realidad. 
  * Independencia: Se supone que cada observación es independiente de cualquier otra. 
  * Características fijas: Las features de entrada se consideran fijas, lo que implica que están libres de errores de medición. 
  * Ausencia de multicolinealidad: No desea características fuertemente correlacionadas, porque esto arruina la estimación de los pesos. 
* **En una situación en la que dos características están fuertemente correlacionadas, se vuelve problemático estimar los pesos porque los efectos de la característica son aditivos y se vuelve indeterminable a cuál de las características correlacionadas atribuir los efectos.**
* El peso de las variables se interpreta por la magnitud de sus beta, pero su importancia por su estimador t, que divide ese beta por el desvío estándar: la intuición es que una variable será más importante cuanto más grande sea el beta pero también cuánto más seguro estemos que ese es el valor real (o sea, más chico el desvío estándar). 
* **El R2 es la proporción de la variabilidad del target que el modelo explica en los datos usados para modelar**, y el R2 ajustado es una medida similar pero que considera la cantidad de variables: ![R^2_{adj} = R^2 * (1-R^2) * \frac{n-1}{n-p-1}](https://render.githubusercontent.com/render/math?math=R%5E2_%7Badj%7D%20%3D%20R%5E2%20*%20(1-R%5E2)%20*%20%5Cfrac%7Bn-1%7D%7Bn-p-1%7D) (para knitear la fórmula ayudó _[esto](https://www.latex4technics.com/)_ y _[esto](https://alexanderrodin.com/github-latex-markdown/)_).
* Las formas de visualizar el valor de los coeficientes puede ser con una especie de boxplot por feature donde cada barra esté centrada en el coeficiente y tenga la amplitud de su desvío estándar (weight plot), o con un boxplot por variable aplicada, donde cada valor de la variable en la muestra se multiplica por el coeficiente estimado y entonces se grafica en forma de boxplot los efectos reales de la variable sobre el target (effect plot). En este último gráfico podría señalarse, por ejemplo, con un color el valor de una observación particular en cada variable para entender, en ese caso, cuál es el valor predicho.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.2.png"> </p>

* A juzgar por lo que debe considerarse como una buena explicación ML, **los modelos lineales no crean las mejores explicaciones**. Son contrastables, pero el valor de referencia sobre el cual tienen interpretación los pesos es un caso en el que todas las variables numéricas son cero y, en forma arbitraria, las categóricas están en sus basales. Ese caso no suele ser real. 
* En ocasiones las variables se **centran en torno de la media**, para que el valor de referencia sea ‘el caso típico’. Esto también podría ser un punto de datos inexistente, pero al menos podría ser más probable o más significativo. Los modelos lineales crean explicaciones verdaderas, siempre que la ecuación lineal sea un modelo apropiado para la relación entre características y resultados. 
* Cuantas más no linealidades e interacciones haya, menos preciso será el modelo lineal y menos sinceras serán las explicaciones.
* Si la interpretabilidad crece cuando las variables son menos, se vuelven útiles métodos como **Lasso**, que realiza la selección de características y la regularización de los pesos de las características seleccionadas. Este modelo incorpora un término de penalización por los valores de los coeficientes, cosa que hace que muchos se ajusten a 0. El **parámetro lambda de penalización** dirá qué tanto se penalizará por cada peso: cuanto más grande sea, cada vez menos características reciben una estimación de peso diferente de cero.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.3.png"> </p>

A la izquierda del gráfico, el parámetro de penalización es muy chico y por lo tanto muchas variables (12) tienen un peso distinto de cero, por eso hay muchas líneas. En cuanto se incrementa el parámetro, más variables van hacia 0. El valor óptimo de lambda se elige muchas veces con validación cruzada. Lasso no es el único medio para reducir la cantidad de features, sino que hay otros como la selección manual, el umbral de correlación (sólo variables que correlacionan en cierta forma con el target) o el forward o backward selection (empezar con una sola o con todas las features, e ir recortando o agregando nuevas y viendo el impacto sobre el R2).
* *En conclusión, el modelo de regresión lineal es muy usado, por lo que en general es aceptado para el modelado predictivo y hacer inferencia. Matemáticamente es sencillo estimar los pesos y tiene la garantía de encontrar pesos óptimos (siempre y cuando los datos cumplen con los supuestos). Junto con los pesos se obtienen intervalos de confianza sustentados por una sólida espalda estadística. Como ventaja, el modelo sirve como base para muchas extensiones, como GLM. Sin embargo, los modelos de regresión lineal solo pueden representar relaciones lineales, es decir, una suma ponderada de las features de entrada. Cada no linealidad o interacción debe ser hecha a mano y definida explícitamente al modelo como una característica de entrada. Como vimos, además, la interpretación de un coeficiente en particular puede ser contraintuitiva, porque es en el contexto del modelo y depende de todas las demás características.*

### 2. Regresión logística
* La regresión lineal falla para clasificación, por el hecho de que su salida no son probabilidades, por lo que no está acotada entre el 0 y el 1. Para subsanar esto es que se usa la **regresión logística** (binomial cuando es para dos clases, multinomial cuando son más), buscando acotar el output de la función entre 0 y 1. La función logística (![\frac{1}{1+e^{-\eta}}](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ceta%7D%7D)) es lo que toma un valor siempre entre 0 y 1, coincidente con 0,5 cuando η es 0. La probabilidad, entonces, es igual a esa fórmula, pero reemplazando η por la expresión con el intercepto y las variables multiplicadas por su coeficiente. 
* La interpretación de los pesos en la regresión logística difiere de la interpretación de los pesos en la regresión lineal, ya que el resultado en la regresión logística es una probabilidad entre 0 y 1. **Los pesos ya no influyen en la probabilidad linealmente.** La suma ponderada se transforma mediante la función logística en una probabilidad. Por lo tanto, necesitamos reformular la ecuación para la interpretación de modo que solo el término lineal esté en el lado derecho de la fórmula: ![\log \frac{P(y=1)}{P(y=0)} = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n](https://render.githubusercontent.com/render/math?math=%5Clog%20%5Cfrac%7BP(y%3D1)%7D%7BP(y%3D0)%7D%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1%20x_1%20%2B%20...%20%2B%20%5Cbeta_n%20x_n). 
* El término dentro del logaritmo son las chances (odds), por lo cual el modelo de regresión logística es un modelo lineal del logaritmo de las odds. **Un cambio de una unidad en una featured cambia el ratio de los logaritmos de los odds, en forma lineal al peso de esa feature.** Si las odds son 2, significa que la probabilidad de y = 1 es el doble de y = 0. Si tiene un peso de 0.7, al aumentar la característica respectiva en una unidad multiplica las probabilidades por e^(0.7) (aproximadamente 2) y las odds cambian a 4. Es importante en la regresión logística comprender el **valor basal**, que es el que toman las variables cuando son 0. El valor del intercepto es, en este caso, útil ya que cuando todas las variables numéricas son cero y las categóricas están en su categoría basal, las odds estimadas son e^β0.
* *Muchos de los pros del modelo lineal de regresión aplican también al modelo logístico de regresión. Como ventaja adicional, por sobre otros modelos de este tipo, es que no solo devuelve la clasificación sino la probabilidad. Es distinto saber que una observación tiene una probabilidad del 99% para una clase, que saber que tiene probabilidad del 51%. Como desventaja particular aparece la pérdida de poder de interpretación de los pesos. Una situación adicional es que, **si hay una característica que separe perfectamente las dos clases, el modelo de regresión logística ya no puede ser entrenado**. Esto se debe a que el peso de esa característica no convergería, porque el peso óptimo sería infinito. Uno podría pensar que esto arruina el problema, pero en caso de que una característica separe perfectamente las dos clases, no es necesario un modelo ML.*

### 3. GLM, GAM y otros modelos
* La característica por excelencia del modelo de regresión lineal es que **la predicción se modela como una suma de características.** Además, requiere de supuestos que muchas veces no se cumplen: puede suceder que E(Y|X) no se distribuya normalmente, por ejemplo. Hay muchos casos en los que la regresión es inútil por diferentes motivos:
  * Si los errores son heterocedásticos o hay outliers, se puede usar regresión robusta.
  * Si lo que se busca es predecir el tiempo hasta la ocurrencia de un evento, se pueden usar diferentes modelos de supervivencia.
  * Si lo que se quiere es predecir una cantidad, se puede usar una regresión de Poisson.
Pero hay otros casos en los que la cuestión radica en el ajuste de los datos a los supuestos propios de la regresión, por lo que puede resolverse relajando la naturaleza de la regresión simple. Recordemos los supuestos clave sobre los datos, que son tres:

<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.4.png"> </p>

* Los requisitos del modelo lineal pueden mostrarse como en el lado izquierdo, y sus casos opuestos como en el derecho. El primer caso sería el caso de no normalidad, y se resuelve con GLM. El segundo sería el caso de variables que interactúan, y se soluciona incorporando interacciones manualmente. El último sería el de relaciones no lineales, que se resuelve con GAM.

#### 3.i. Resultados no gaussianos - GLM

* Los casos en los que la variable respuesta (condicionada a los valores de las features) no sigue una distribución normal pueden ser múltiples: variable respuesta de un tipo determinado (conteo, categoría, cantidad de tiempo) o simplemente una variable numérica, pero con una forma de distribución diferente (como las que se refieren al ingreso). **El concepto central de cualquier GLM es: mantener la suma ponderada de las características, pero permitir distribuciones de resultados no gaussianas y conectar la media esperada de esta distribución y la suma ponderada a través de una función posiblemente no lineal.** La regresión logística es un ejemplo: supone una distribución de Bernoulli para el target y vincula la media esperada y la suma ponderada utilizando la función logística. 
* La expresión, g(E_Y(y|x)) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n, contiene tres elementos: la parte derecha que es igual que en la regresión lineal (llamada predictor), y del lado izquierdo una función link g() y una E(y|x), que debe tener forma exponencial. Hay muchas distribuciones [de tipo exponencial](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions/). **Si la distribución es gaussiana y la función es identidad, estamos en la regresión lineal simple**; si la distribución es de Bernoulli y la función es logit estamos ante una regresión logística. Como en la logística, los GLM cambian su interpretación con respecto a los coeficientes, que en algunos casos pueden ser más sencillos de interpretar y en otros más complejos.

#### 3.ii. Interacciones

* En ocasiones, el efecto de una feature sobre la respuesta no es independiente del valor de otras features. En estos casos es útil incluir **interacciones entre las variables**, es decir crear nuevas variables dadas por combinaciones de ambas. En el caso de las variables numéricas el método es sencillo, y es el de multiplicar una por otra. Interpretando una salida de un modelo construido así, el efecto de cada variable sobre la respuesta puede dejar de ser lineal, sino que ser diferente según el valor (o la activación o no) de otra variable. De este modo, la relación deja de ser estrictamente lineal.

#### 3.iii. No linealidades

* La linealidad en los modelos significa que **no importa qué valor tenga una observación en una característica particular, aumentar el valor en una unidad siempre tiene el mismo efecto en el resultado previsto**. Esto claramente no se verifica en la práctica, y posiblmente sea el menos realista de los supuestos. Existen tres formas de subsanar esta característica de los datos (que puede observarse cuando la nube de puntos de la variable con respecto al target no dibuja una línea).
  * **Transformación de variables**: Aplicar el logaritmo, suavizando la relación entre las variables. Esto hace que la relación deje de ser constante, interpretándose como "Si el logaritmo de la característica se incrementa en uno, la predicción se incrementa en el peso correspondiente".
  * **Categorización de variables**: Generar variables específicas para cuando el valor está en ciertos rangos, lo cual 'des-linealiza' la relación. Sin embargo, es muy probable caer en overfitting y no está demasiado claro como discretizar una variable contínua.
  * **GLM**: Sin duda, la solución más completa es la de aceptar la no-linealidad y trabajar con ello: relajar la restricción de una suma de pesos lineales, para trabajar con una suma de funciones: ![g(E_Y(y|x)) = \beta_0 + f_1(x_1) + ... + f_n(x_n)](https://render.githubusercontent.com/render/math?math=g(E_Y(y%7Cx))%20%3D%20%5Cbeta_0%20%2B%20f_1(x_1)%20%2B%20...%20%2B%20f_n(x_n)). GAM sigue siendo una suma de efectos de features, pero tiene la opción de **permitir relaciones no lineales entre algunas features y el target**. Lo importante en este caso es cómo aproximar la función, para lo que se usan splines, curvas diferenciables definidas en porciones mediante polinomios. La salida del modelo asignará pesos a cada spline. 
  
* *En síntesis, las extensiones del modelo lineal son enormes, y resuelven casi todos los problemas que pueden tener los datos con respecto a los modelos lineales. Muchos investigadores y profesionales de la industria tienen mucha experiencia con modelos lineales y los métodos son aceptados en muchas comunidades como status quo para el modelado. Los softwares estadísticos generalmente tienen interfaces realmente buenas para adaptarse a GLM, GAM y modelos lineales más especiales. Suponiendo que los modelos lineales son altamente interpretables pero a menudo no se ajustan a la realidad, las extensiones descritas en este capítulo ofrecen una buena manera de lograr una transición suave a modelos más flexibles, al tiempo que conservan algo de la capacidad de interpretación. Sin embargo, como se dijo, la mayoría de las modificaciones del modelo lineal hacen que el modelo sea menos interpretable. Cualquier función de enlace (en un GLM) que no sea la función de identidad complica la interpretación; las interacciones también complican la interpretación; los efectos de características no lineales son menos intuitivos (como la transformación logarítmica) o ya no se pueden resumir en un solo número (por ejemplo, funciones de spline).*

### 4. Árboles de decisión
* La regresión lineal y los modelos de regresión logística fallan en situaciones donde la relación entre las características y el resultado es no lineal o donde las características interactúan entre sí. **Los modelos basados en árboles dividen los datos varias veces de acuerdo con ciertos valores de corte en las características.** 
* A través de la división, se crean diferentes subconjuntos del conjunto de datos, y cada observación pertenece a un subconjunto. Los subconjuntos finales se denominan **nodos terminales o de hoja** y los subconjuntos intermedios se denominan **nodos internos o nodos divididos**. Para predecir el resultado en cada nodo hoja, se utiliza el resultado promedio de los datos de entrenamiento en este nodo. Los árboles se pueden usar para clasificación y regresión. La fórmula que describe los árboles es ![\hat{y} = \hat{f}(x) = \sum_{m=1}^{M} c_mI \left\lbrace x \varepsilon R_m \right\rbrace](https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D%20%3D%20%5Chat%7Bf%7D(x)%20%3D%20%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20c_mI%20%5Cleft%5Clbrace%20x%20%5Cvarepsilon%20R_m%20%5Cright%5Crbrace). Cada observacion cae exactamente en un nodo hoja (subconjunto Rm). I (y lo que tiene dentro) es la función de identidad que retorna 1 si x está en el subset Rm y 0 en otros casos. Si una instancia cae en un nodo hoja Ri, el resultado previsto es ci, promedio de todas las instancias de entrenamiento en ese nodo. 
* Para la determinación de los puntos de corte, el elemento central es el **algoritmo CART**: CART toma una función y determina qué punto de corte minimiza la varianza de y en regresión (es decir, qué tanto los valores y en un nodo se distribuyen alrededor de su valor medio) o el índice de Gini en clasificación (qué tan "impuro" es un nodo, si todas las clases tienen la misma frecuencia, el nodo es impuro, si solo hay una clase presente, es máximo puro). La varianza y el índice de Gini se minimizan cuando las observaciones en los nodos tienen valores muy similares para y. Como consecuencia, **el mejor punto de corte hace que los dos subconjuntos resultantes sean lo más diferentes posible con respecto al resultado objetivo**. El algoritmo continúa esta búsqueda y división recursivamente en ambos nodos nuevos hasta que se alcanza un criterio de detención. Los criterios posibles son: Un número mínimo de observaciones que deben estar en un nodo antes de la división, o el número mínimo de observaciones que deben estar en un nodo terminal. 
* La importancia general de una feature en un árbol de decisión nos dice cuánto ayudó una característica a mejorar la pureza de todos los nodos, y se puede calcular de la siguiente manera: revise todas las divisiones para las que se utilizó la característica y mida cuánto ha reducido la varianza o el índice de Gini en comparación con el nodo primario. La suma de todas las importancias se escala a 100. Esto significa que cada importancia puede interpretarse como parte de la importancia general del modelo.
* *En síntesis, **la estructura de árbol es ideal para capturar interacciones entre características en los datos**. La estructura de árbol también tiene una visualización natural, con sus nodos y bordes. La estructura de árbol invita automáticamente a pensar en los valores pronosticados para instancias individuales como contrafactuales: “Si una entidad hubiera sido mayor / menor que el punto de división, la predicción habría sido y1 en lugar de y2". Además, a diferencia de los modelos lineales, nunca hay necesidad de transformar características. Sin embargo, los árboles no pueden lidiar con relaciones lineales. Esto va de la mano con la falta de suavidad. Los cambios leves en la función de entrada pueden tener un gran impacto en el resultado previsto, lo que generalmente no es deseable. Los árboles también son bastante inestables. Algunos cambios en el conjunto de datos de entrenamiento pueden crear un árbol completamente diferente. Los árboles de decisión son muy interpretables, siempre que sean cortos. El número de nodos terminales aumenta rápidamente con la profundidad.*

### 5. Reglas de decisión
* Una regla de decisión es una **cláusula IF-THEN**. Su estructura IF-THEN se asemeja semánticamente al lenguaje natural y a la forma en que pensamos, siempre que la condición se construya a partir de características inteligibles. 
* La utilidad de una regla de decisión generalmente se resume en dos números: Soporte y precisión. El soporte es el porcentaje de instancias a las que se aplica la condición de una regla se denomina soporte. La precisión, en cambio, es una medida de la precisión de la regla al predecir la clase correcta para las instancias a las que se aplica la condición de la regla. Por lo general, existe un trade off entre precisión y soporte: al agregar más características a la condición, podemos lograr una mayor precisión, pero perdemos soporte. 
* Una lista de decisiones introduce un orden a las reglas de decisión: si la condición de la primera regla es verdadera para una instancia, usamos la predicción de la primera regla; si no, pasamos a la siguiente regla y verificamos si corresponde y así sucesivamente. Un conjunto de decisiones se asemeja a una democracia de las reglas, excepto que algunas reglas pueden tener un mayor poder de voto. La regla predeterminada es la regla que se aplica cuando no se aplica ninguna otra regla. Hay muchas maneras de aprender reglas de los datos y este libro está lejos de abarcarlas todas. Este capítulo te muestra tres de ellos.

  * **OneR aprende las reglas de una sola feature**. OneR se caracteriza por su simplicidad, interpretabilidad y su uso como benchmark. De todas las características, OneR selecciona la que lleva más información sobre el resultado de interés y crea reglas de decisión a partir de esta característica. El algoritmo es simple y rápido: discretiza las características continuas eligiendo los intervalos apropiados. Para cada característica, cree una tabla cruzada entre los valores de la característica y el resultado (categórico), de la que crea la regla de decisión. Calcula el error total de las reglas para la función, y luego selecciona la función con el error total más pequeño.
  
  * **Sequential covering es un procedimiento general que aprende de forma iterativa las reglas y elimina las observaciones cubiertos por la nueva regla.** Este procedimiento es utilizado por muchos algoritmos de aprendizaje de reglas. La idea es simple: primero, encuentre una buena regla que se aplique a algunas de las observaciones. Elimine ellas, independientemente de si los puntos se clasifican correctamente o no. Repita el aprendizaje de reglas y la eliminación de puntos cubiertos con los puntos restantes hasta que no queden más puntos o se cumpla otra condición de detención. El resultado es una lista de decisiones. Este enfoque de aprendizaje de reglas repetido y eliminación de observaciones cubiertas se denomina "separar y conquistar". Para múltiples clases, el enfoque debe ser modificado: el algoritmo de cobertura secuencial comienza con la clase menos común, aprende una regla para ello, elimina todas las instancias cubiertas, luego pasa a la segunda clase menos común y así sucesivamente. Existen muchos algoritmos para definir cuál es la regla que se creará, porque este proceso comienza por una regla.
  
  * **Bayesian Rule Lists combinan patrones frecuentes pre-minados en una lista de decisiones utilizando estadísticas bayesianas**. Un patrón frecuente es la frecuente coaparición de valores de features. Como un paso de preprocesamiento para el algoritmo BRL, utilizamos las características y extraemos patrones que ocurren con frecuencia en conjunto. Existen muchos algoritmos para encontrar patrones tan frecuentes, por ejemplo, Apriori o FP-Growth. El objetivo del algoritmo BRL es aprender una lista de decisiones precisa utilizando una selección de las condiciones predeterminadas, al tiempo que prioriza listas con pocas reglas y condiciones cortas. El algoritmo comienza con patrones de valor de característica de pre-minería con el algoritmo FP-Growth. BRL hace una serie de suposiciones sobre la distribución del target y la distribución de los parámetros que definen la distribución del target (esa es la estadística bayesiana). Las estimaciones en las estadísticas bayesianas siempre son un poco complicadas, porque generalmente no podemos calcular directamente la respuesta correcta, pero tenemos que dibujar candidatos, evaluarlos y actualizar nuestras estimaciones posteriores utilizando Cadenas de Markov. Los autores de BRL proponen primero dibujar una lista de decisión inicial y luego modificarla iterativamente para generar muestras de listas de decisión a partir de la distribución posterior de las listas (Markov).

* *En síntesis, las reglas IF-THEN son fáciles de interpretar. Son probablemente el más interpretable de los modelos interpretables. Las reglas de decisión pueden ser tan expresivas como los árboles de decisión, a la vez que son más compactas. La predicción con las reglas IF-THEN es rápida, ya que solo se necesitan verificar unas pocas declaraciones binarias para determinar qué reglas se aplican. Las reglas de decisión son robustas frente a las transformaciones monótonas de las características de entrada, porque solo cambia el umbral en las condiciones. También son robustos frente a los valores atípicos, ya que solo importa si una condición se aplica o no. Como contras, se puede ver que la investigación y la literatura para las reglas IF-THEN se enfoca en la clasificación y descuida casi por completo la regresión. A menudo, las características también tienen que ser categóricas. Eso significa que las características numéricas deben clasificarse si desea utilizarlas: la categorización de características continuas es un problema no trivial que a menudo se descuida. Las reglas de decisión son malas al describir relaciones lineales entre características y resultados. Ese es un problema que comparten con los árboles de decisión. Los árboles de decisión y las reglas solo pueden producir funciones de predicción escalonadas, donde los cambios en la predicción siempre son pasos discretos y nunca curvas suaves. Esto está relacionado con el problema de que las entradas tienen que ser categóricas.*

### 6. Rulefit
* El algoritmo **RuleFit** aprende modelos lineales dispersos que incluyen efectos de interacción detectados automáticamente en forma de reglas de decisión. RuleFit aprende un modelo lineal disperso con las features originales y también con una serie de características nuevas que son reglas de decisión. Estas nuevas características capturan interacciones entre las features originales. RuleFit genera automáticamente estas características a partir de los árboles de decisión. 
* Cada ruta a través de un árbol se puede transformar en una regla de decisión combinando las decisiones divididas en una regla. **Cada árbol se descompone en reglas de decisión que se utilizan como características adicionales en un modelo de regresión lineal disperso, que utiliza Lasso**. La interpretación es análoga a los modelos lineales: el resultado predicho cambia tanto como los coeficientes, si la feature cambia en una unidad, siempre que todas las demás características permanezcan sin cambios. 
* **RuleFit agrega automáticamente interacciones de features a modelos lineales**. Por lo tanto, resuelve el problema de los modelos lineales donde las interacciones deben agregarse manualmente. Además, las reglas creadas son fáciles de interpretar, porque son reglas de decisión binarias. La regla se aplica a una observación o no. La buena interpretación solo se garantiza si el número de condiciones dentro de una regla no es demasiado grande. Una regla con 1 a 3 condiciones me parece razonable. Esto significa una profundidad máxima de 3 para los árboles en el conjunto de árboles.

### 7. Otros
* La lista de modelos interpretables está en constante crecimiento y de tamaño desconocido. Incluye modelos simples como modelos lineales, árboles de decisión y bayesianos, pero también modelos más complejos que combinan o modifican modelos de aprendizaje automático no interpretables para hacerlos más interpretables.
* En el modelo **Naive Bayes**, por ejemplo, la probabilidad condicional de una clase es la probabilidad de clase multiplicada por la probabilidad de cada feature dada la clase, normalizada por Z. Esta fórmula puede derivarse utilizando el teorema de Bayes. Se trata de un modelo interpretable debido al supuesto de independencia: está muy claro para cada característica cuánto contribuye a una determinada predicción de clase, ya que podemos interpretar la probabilidad condicional.
* El método de **vecino más cercano** se puede usar para regresión y clasificación y utiliza los vecinos más cercanos de una observación para la predicción. Para la clasificación, el método vecino más cercano a k asigna la clase más común de los vecinos más cercanos de una instancia. Para la regresión, toma el promedio del resultado de los vecinos. Este modelo difiere de los otros modelos interpretables presentados en este libro porque es un algoritmo de aprendizaje basado en instancias.

## 5. Métodos de interpretación agnósticos al modelo

* Considerar a la **instancia de interpretación** como independiente del modelo en sí es útil por la flexibilidad: todo lo que contribuye a una interpretación de un modelo de aprendizaje automático, como una interfaz gráfica o de usuario, se vuelve independiente del modelo subyacente.
* Como se vio, utilizar solo modelos interpretables a menudo tiene la gran desventaja de que se pierde el rendimiento predictivo en comparación con otros modelos de aprendizaje automático y se limita a un tipo de modelo. Utilizar métodos de interpretación específicos del modelo también es algo que se vincula a un tipo de modelo y será difícil cambiar a otra cosa.
* En la abstracción dada por la jerarquía de capas **Mundo - Datos - Modelo - Humanos**, el método de interpretación cae entre el modelo y el humano, ayudándolo a lidiar con la opacidad de los modelos de aprendizaje automático. _¿Cuáles fueron las características más importantes para un diagnóstico particular? ¿Por qué una transacción financiera se clasificó como fraude?_
Lo que sigue, entonces, es una serie de métodos de interpretación agnósticos al modelo:

### 1. Gráfico de dependencia parcial (PDP)
* Es un gráfico que muestra el efecto marginal que una o dos características tienen sobre el resultado previsto de un modelo de aprendizaje automático. Un gráfico de dependencia parcial puede mostrar si la relación entre el objetivo y una característica es lineal, monótona o más compleja. Al marginar sobre las otras características, obtenemos una **función que depende solo de las características en un subconjunto que nos interesa**. 
* El gráfico tendrá la forma de una línea para variables contínuas, o de varias barras para variables categóricas. Si lo que se quiere es graficar la relación entre dos variables y el target, en general se usa un gráfico de dos ejes donde la combinación tiene una diferente intensidad (en general, de color) según la probabilidad o el valor estimado.
* **Si la característica para la que calculó el PDP no está correlacionada con las otras características, entonces los PDP representan perfectamente cómo la característica influye en la predicción en promedio**. Como sabemos, la relación es causal para el modelo pero no necesariamente para el mundo real. Como contras, además, se puede decir que el número máximo realista de características en una función de dependencia parcial es dos. Cuando las variables están correlacionadas, la interpretación es mucho más dificil. La suposición de independencia es el mayor problema con este tipo de gráficos. Cuando hay dependencia, lo que se hace es bastante irreal: se crean nuevas observaciones en áreas de la distribución de características donde la probabilidad real es muy baja. El otro problema que tiene este gráfico es el problema propio de los promedios, ya que asume que el efecto es igual para todas las observaciones.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/4.1.png" height="502" width="602"> </p>

### 2. Gráfico de esperanzas condicionales individuales
* Este gráfico muestra **una línea/barra por observación**, dejando ver cómo cambia la predicción de la instancia cuando cambia una feature. Soluciona el problema del PDP de promediar, ya que en este caso muestra todas las observaciones del grupo. Es el gráfico del que PDP arma el promedio. PDP es útil en la medida que no haya correlación entre la característica y el resto: si la hubiera, un gráfico de este tipo aportará mucha más información. Este gráfico podría mostrar excepciones, o casos en los que un comportamiento no vale para el grupo entero sino para una parte. Incorporarle al gráfico su promedio (o sea PDP) o bien graficar la curva pero pensando en la derivada de la predicción respecto de la feature.
* Si bien se trata de un gráfico más intuitivo que PDP, sigue graficando de a una variable a la vez y puede que se vuelva superpoblado de existir muchas observaciones.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/4.2.png" height="502" width="602"> </p>

### 3. Gráfico de efectos locales acumulados (ALE)
* Al igual que los gráficos PDP, este gráfico busca entender **cómo una feature afecta la predicción en promedio**. Sin embargo, cuando las variables están correlacionadas, al graficar en dos dimensiones PDP tiene un problema grave. Para calcular el efecto de feature en un valor, el PDP reemplaza esa feature en todas las observaciones por ese valor, suponiendo falsamente que la distribución es igual en todos los puntos. Esto da como resultado combinaciones poco probables de x1 y x2, que el PDP utiliza para el cálculo del efecto promedio.
* La solución podría venir por medio de los **M-Plots**, que promedian las predicciones para ese valor de la feature. Los M-Plots evitan promedios de predicciones de instancias de datos poco probables, pero mezclan el efecto de una feature con los efectos de todas las feature correlacionadas.
* Las **gráficas ALE** resuelven este problema calculando, también en función de la distribución condicional de las características, las **diferencias en las predicciones en lugar de los promedios**. Primero, dividimos la entidad en intervalos (líneas verticales). Para las instancias de datos (puntos) en un intervalo, calculamos la diferencia en la predicción cuando reemplazamos la función con el límite superior e inferior del intervalo (líneas horizontales). Estas diferencias se acumulan y centran más tarde, lo que resulta en la curva ALE.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/4.3.png" height="502" width="602"> </p>
* Si bien tiene sus desventajas, en general **se prefiere este gráfico a PDP**.

### 4. Estadístico H
* Cuando las features interactúan entre sí, la descripción de un modelo no puede ser armada mirando la suma de los efectos de las features, ya que **el efecto de una depende del valor de la otra**. Estadísticamente, si las features no interactúan se puede descomponer la función de dependencia parcial, escribiendo la función de predicción como una suma de funciones de dependencia parcial. La proporción de variabilidad que explica esta suma de funciones de dependencia parcial es, también, una forma de ver si hay o no hay interacción. El **estadístico H de Friedman** sirve para ese propósito. Hay que tener en cuenta que el esfuerzo para calcular este estadístico crece exponencialmente con la cantidad de features. Las estadísticas H pueden ser mayores que 1, lo que dificulta la interpretación.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/4.4.png" height="502" width="602"> </p>

* Menos del 10% de la varianza de cada variable es explicada por el resto, por lo que hay poca interacción. Si la hubiera, **podría seleccionarse una de las variables y ver la composición, viendo cuál del resto de las variables es con la que más se asocia**.
* El estadístico H tiene una interpretación significativa: la interacción se define como la parte de la varianza que se explica por la interacción. Con la estadística H también es posible analizar interacciones arbitrarias superiores, como la fuerza de **interacción entre 3 o más características**. Como contra, se puede decir que es difícil decir cuándo la estadística H es lo suficientemente grande como para considerar una interacción "fuerte".

### 5. Importancia de la permutación de features
* El concepto de este estadístico es que **una característica es "importante" si modificar sus valores aumenta el error del modelo, lo que implica que el modelo se basó en la feature para la predicción.** Una característica no es importante, en cambio, si mezclar sus valores deja el error del modelo sin cambios, porque en este caso el modelo ignoró la característica para la predicción.
* Fisher, Rudin y Dominici (2018) sugieren en su artículo dividir el conjunto de datos a la mitad e **intercambiar** los valores de la variable entre uno y el otro, en vez de cambiarlo directamente para todo el dataset.
* Una cuestión importante de esta técnica es el lugar en el que ver esto: ¿debe hacerse en training o en testing? Como la importancia de las variables depende de las estimaciones del error, que en training 'no sirven' por fuera de la muestra, uno podría decir que deberíamos hacerlo en test. Sin embargo, a diferencia de testing, la importancia de la feature basada en los datos de entrenamiento nos dice qué características son importantes para el modelo en el sentido de que depende de ellas para hacer predicciones. Al final, se debe **decidir si desea saber cuánto depende el modelo de cada característica para hacer predicciones (-> datos de entrenamiento) o cuánto contribuye la característica al rendimiento del modelo en datos no vistos (-> datos de prueba).**
* Entre las ventajas de esta técnica está que es clara: la importancia de la característica es el aumento en el error del modelo cuando se destruye la información de la característica. La medida de importancia tiene en cuenta automáticamente todas las interacciones con otras características. Además, la importancia de la característica de permutación **no requiere volver a entrenar el modelo.** Sin embargo, como principal problema está la cuestión del lugar para probarlo: training o test puede traer conclusiones muy disímiles. A diferencia de otras técnicas, para llegar a esto es necesario el verdadero valor del target, cosa que no siempre está disponible en esta instancia. Si hay una correlación muy alta es posible que ganen fuerza eventos muy contrafácticos.

### 6. Sustituto global
* La técnica de los sustitutos globales es la de **reemplazar un modelo 'caja negra' por otro interpretable**, entrenado para aproximar las predicciones del otro. El propósito de los estos modelos sustitutos es aproximar las predicciones del modelo subyacente con la mayor precisión posible y ser interpretables al mismo tiempo. La técnica es, una vez armado un modelo caja negra, **entrenar uno interpretable teniendo las predicciones de la caja negra como target**.
* El rendimiento del modelo de caja negra no juega un papel en el entrenamiento del modelo sustituto. La interpretación del modelo sustituto sigue siendo válida porque hace declaraciones sobre el modelo y no sobre el mundo real. Pero, por supuesto, la interpretación del modelo sustituto se vuelve irrelevante si el modelo de caja negra es malo, porque entonces el modelo de caja negra en sí es irrelevante.
* Con el R2 usando de referencia el modelo caja negra en vez del promedio simple, podemos medir con facilidad la capacidad de aproximación. Este método es flexible porque puede usarse cualquier modelo interpretable, y también es intuitivo. Sin embargo, esta técnica **absorbe también las desventajas de ambos modelos.**

### 7. Sustituto local (LIME)
* Si para una observación puntual quisiera hacerse el ejercicio de simplificación de un modelo caja negra, podría tomarse esa observación y perturbarse el dataset tomando, para cada caso, la predicción que hace el modelo. Si luego de ello se entrena ese dataset  perturbado en un modelo interpretable cualquiera ponderando por la proximidad a los datos originales, el resultado es un modelo local interpretable.
* Este método funciona bien con datos tabulares, imágenes o texto: prácticamente con todo.

### 8. Reglas de alcance (Anclas)
* Estas reglas buscan reducir al mínimo la cantidad de features que, inalterables, generan determinada predicción. La idea es una regla IF-THEN basada en combnaciones de features: se elige una cantidad reducida de features, asegurandose que una modificación en el resto no tenga impacto en el valor predicho.

### 9. Valores de Shapley
* Una predicción puede explicarse suponiendo que **cada valor de feature en una observación es un jugador en un juego donde la predicción es el pago.** Los valores de Shapley nos dicen cómo distribuir equitativamente el "pago" entre las características.
* El valor de Shapley es la contribución marginal promedio de un valor de feature en todas las coaliciones posibles. En las observaciones disponibles, existe una cantidad N de combinaciones que incluyen a cada feature en diferentes estados. Si una feature es binaria, y se tienen los dos casos (todas las variables iguales y 1, todas las variables iguales y 0) entonces se puede encontrar ese valor diferencia. Si se promedia (en forma ponderada por su nivel de aparición) eso para todos los casos de todas las variables iguales, se encuentra el valor de Shapley para esa variable. **Si estimamos los valores de Shapley para todos los valores de características, obtenemos la distribución completa de la predicción (menos el promedio) entre los valores de características.**
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/4.5.png" height="502" width="602"> </p>

* La interpretación del valor de Shapley para el valor de característica j es: **El valor de la característica j contribuyó en esta medida a la predicción de esta instancia particular en comparación con la predicción promedio para el conjunto de datos.**
* El valor de Shapley es el único método de atribución que satisface las propiedades **eficiencia** (las contribuciones de features deben sumar las diferencias de predicción en promedio), **simetría** (si dos características contribuyen en forma similar a todas las coaliciones, su valor debería ser similar), **dummy** (una característica que no cambia el valor predicho debería tener valor de Shapley 0) y **aditividad** (en algoritmos como random forest, el valor de una variable en el arbol completo debería ser igual a la suma de los árboles individuales), que juntas pueden considerarse una definición de pago justo.
* El valor de Shapley podría ser el único método para entregar una explicación completa. En situaciones donde la ley requiere explicabilidad, como el "derecho a explicaciones" de la UE, **el valor de Shapley podría ser el único método legalmente compatible**, porque se basa en una teoría sólida y distribuye los efectos de manera justa. Métodos como LIME suponen un comportamiento lineal del modelo de aprendizaje automático a nivel local, pero no hay una teoría de por qué esto debería funcionar. El valor de Shapley requiere mucho tiempo de computación: un cálculo exacto del valor de Shapley es **computacionalmente costoso** porque hay 2^n coaliciones posibles de los valores de la característica y la "ausencia" de una característica tiene que ser simulada dibujando instancias aleatorias, lo que aumenta la varianza para la estimación de la estimación de los valores de Shapley. El valor de Shapley puede malinterpretarse: no es la diferencia del valor pronosticado después de eliminar la característica del entrenamiento del modelo.

### 10. SHAP (SHapley Additive exPlanations)
* El objetivo de SHAP es explicar la predicción de una observación x calculando la contribución de cada feature a la predicción. El método de explicación SHAP calcula los valores de Shapley, pero explicadas como un método de atribución de características aditivas, un modelo lineal. Esa vista **conecta los valores LIME y Shapley.** La idea de SHAP es usar los valores de Shapley en la diagonal y a eso incorporarle elementos dados por los efectos de las features:
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/4.6.png" height="502" width="602"> </p>

## 6. Explicaciones basadas en ejemplos
* A diferencia de los ejemplos anteriores que explican el modelo a partir de un resumen de sus características, las explicaciones basadas en ejemplos **toman una serie de observaciones y las usan para hacerlo interpretable.** Por lo tanto, solo tienen sentido si podemos representar una instancia de los datos de una manera humanamente comprensible. 
* Esto es más útil en imágenes, por ejemplo, que en datos tabulares donde una observación puede constar de cientos o miles de features (menos estructuradas).
* El plan de las explicaciones basadas en ejemplos es: **La cosa B es similar a la cosa A y A causó Y, por lo que predigo que B también causará Y.**
* Hay varios métodos de explicaciones basadas en ejemplos: **contrafactuales**, que nos dicen cómo debe cambiar una instancia para cambiar significativamente su predicción; **adversarios** con énfasis en voltear la predicción y no explicarla; **prototipos** seleccionando instancias representativas de los datos; **influyentes** son los puntos de datos de entrenamiento que fueron los más influyentes para los parámetros de un modelo de predicción.

### 1. Explicaciones contrafácticas
* La base para los contrafácticos es la idea que si X no hubiera ocurrido, entonces tampoco lo hubiera hecho Y. La relación entre las entradas y la predicción es muy simple: **los valores de las features causan la predicción.** Incluso si en realidad esa relación podría no ser causal, podemos ver las entradas de un modelo como la causa de la predicción.
* Una explicación contrafáctica de una predicción describe el cambio más pequeño en los valores de la característica que cambia la predicción a una salida predefinida. *¿Cuál es el cambio más pequeño en las características (ingresos, número de tarjetas de crédito, edad, ...) que cambiaría la predicción de rechazado a aprobado?*
* Los contrafactuales son explicaciones amigables para los humanos, porque son contrastantes con la observación actual y porque generalmente se centran en un pequeño número de cambios de características. Pero los contrafácticos sufren el **"efecto Rashomon"**, una película japonesa en la que diferentes personas explican la muerte de un samurai igualmente bien, pero las historias se contradicen entre sí.
* La interpretación de las explicaciones contrafácticas es muy clara: si los valores de característica de una instancia se cambian de acuerdo con el contrafactual, la predicción cambia a la predicción predefinida. El método contrafáctico crea una nueva instancia, pero también podemos resumir un contrafactual al informar qué valores de características han cambiado. El método contrafactual no requiere acceso a los datos o al modelo. Una empresa tiene interés en proteger el modelo y los datos debido a secretos comerciales o razones de protección de datos. 

### 2. Explicaciones contrafácticas
* La explicación a través de la confrontación (ejemplos adversarios) es perturbar una observación de forma intencional para que el modelo de aprendizaje automático haga una predicción falsa. Los ejemplos adversarios son ejemplos contrafácticos con el objetivo de engañar al modelo, no interpretarlo.
* Existen muchas técnicas para **crear ejemplos adversos**. La mayoría de los enfoques sugieren minimizar la distancia entre el ejemplo de confrontación y la instancia a manipular, mientras se cambia la predicción al resultado deseado (de confrontación).

### 3. Prototipos y críticas
* Más allá de las técnicas de explicación, estas técnicas sirven para ilustrar qué observaciones son 'buenas' y cuáles son complicadas para el modelo. Un **prototipo** es una observación representativa de todos los datos. Una **crítica** es una instancia de datos que no está bien representada por el conjunto de prototipos. Los prototipos y las críticas se pueden usar independientemente de un modelo de aprendizaje automático para describir los datos, pero también se pueden usar para crear un modelo interpretable o para hacer que un modelo de caja negra sea interpretable.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/6.1.jpg" height="250" width="600"> </p>

* Se trata de un modelo bastante libre con un algoritmo fácil de implementar, que en algunos casos es superador a las observaciones aleatorias de una clase. A pesar de esto, se trata de una técnica de la que hay que elegir múltiples parámetros.

### 4. Observaciones influyentes
* Al ser los modelos consolidaciones de las variables de múltiples observaciones, la eliminación de una **observación influyente** podría tener un impacto fuerte. Al identificar observaciones de entrenamiento influyentes, podemos "depurar" los modelos de aprendizaje automático y explicar mejor sus comportamientos y predicciones. Como se sabe, incluir o no una observación influyente, si esta es outlier, puede modificar mucho la predicción del modelo.
* Preguntar cómo cambiarían los parámetros del modelo o las predicciones si eliminamos observaciones del entrenamiento contrasta con otros enfoques de interpretabilidad que analizan cómo cambia la predicción cuando manipulamos las features de las observaciones a predecir, como los gráficos de dependencia parcial o la importancia de la feature. Con instancias influyentes, **no tratamos el modelo como fijo, sino como una función de los datos de entrenamiento.**
* **DFBETA** mide el efecto de eliminar una observación en los parámetros del modelo. La **distancia de Cook** (Cook, 197763) mide el efecto de eliminar una observación en las predicciones del modelo. Para ambas medidas tenemos que volver a entrenar el modelo repetidamente, omitiendo observaciones individuales cada vez. Los parámetros o predicciones del modelo con todas las observaciones se comparan con los parámetros o predicciones del modelo con una de las observaciones eliminadas de los datos de entrenamiento. Una influencia de 0.01 significa que si eliminamos la observación, la probabilidad pronosticada cambia en 1 punto porcentual en promedio.
* La **función de influencia** es un método similar pero que no necesita reentrenamientos, ya que mide qué tan fuertemente los parámetros del modelo o las predicciones dependen de una observación de entrenamiento. En lugar de eliminar la instancia, el método compensa la instancia en la pérdida en un paso muy pequeño. Este método implica aproximar la pérdida alrededor de los parámetros del modelo actual usando el gradiente y la matriz de Hesse.

## 7. Redes neuronales
* El **aprendizaje profundo** ha sido muy exitoso, especialmente en tareas que involucran imágenes y textos como la clasificación de imágenes y la traducción de idiomas. La historia de éxito de las redes neuronales profundas comenzó en 2012, cuando el desafío 65 de clasificación de imágenes ImageNet se ganó con un enfoque de este tipo. Desde entonces, hemos sido testigos de una explosión cámbrica de arquitecturas de redes neuronales profundas, con una tendencia hacia redes más profundas con más y más parámetros de peso.
* Para hacer predicciones con una red neuronal, la entrada de datos se pasa a través de muchas capas de multiplicación con los pesos aprendidos y a través de transformaciones no lineales. **Una sola predicción puede involucrar millones de operaciones matemáticas dependiendo de la arquitectura de la red neuronal.** No hay posibilidad de que los humanos podamos seguir el mapeo exacto desde la entrada de datos hasta la predicción.
* Las redes neuronales profundas aprenden características de alto nivel en las capas ocultas. Esta es una de sus mayores fortalezas y reduce la necesidad de ingeniería de características. Con redes neuronales convolucionales, la imagen se alimenta a la red en su forma cruda (píxeles). **La red transforma la imagen muchas veces.** Primero, la imagen atraviesa muchas capas convolucionales. En esas capas convolucionales, la red aprende características nuevas y cada vez más complejas. Luego, la información de la imagen transformada atraviesa las capas completamente conectadas y se convierte en una clasificación o predicción.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/7.1.jpg" height="402" width="602"> </p>

* Hay muchas formas de explicitar las features que se aprenden. Sin embargo, por la estructura de las redes podría pensarse qué es lo que aprende: A) Neurona de convolución, B) Canal de convolución, C) Capa de convolución, D) Neurona, E) Capa oculta, F) Neurona de probabilidad de clase.
* Dada la complejidad y la opacidad de las redes neuronales, la visualización de características es un **paso importante en el análisis y la descripción de las redes neuronales**. Algunas características son abstractas, por lo que no hay palabras o conceptos mentales. Las visualizaciones de características pueden transmitir la ilusión de que entendemos lo que está haciendo la red neuronal. ¿Pero **realmente entendemos lo que está sucediendo en la red neuronal?** Incluso si observamos cientos o miles de visualizaciones de características, no podemos entender la red neuronal. Los canales interactúan de manera compleja, las activaciones positivas y negativas no están relacionadas, las neuronas múltiples pueden aprender características muy similares y para muchas de las características no tenemos conceptos humanos equivalentes. **No debemos caer en la trampa de creer que entendemos completamente las redes neuronales**.
