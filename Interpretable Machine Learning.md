_[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)_

## 1. Introducción

* El libro busca hacer que los modelos ML sean fáciles de interpretar. Por lo tanto, la organización de los diferentes modelos que hay será por su capacidad de ser interpretados.
* **ML: conjunto de métodos que usan las computadoras para hacer y mejorar predicciones o comportamientos basados en datos.**
* El libro se ocupa del aprendizaje supervisado. La serie de pasos es la siguiente:
  1. **Recoger** datos pasados con el target.
  2. **Llevar** esos datos a un algoritmo ML.
  3. **Integrar** el resultado de ese algoritmo a un producto o proceso.

## 2. Interpretabilidad

* La **interpretabilidad** es el grado en que un humano puede comprender la causa de una decisión.
* Es posible que algunos modelos no requieran explicaciones porque se usan en un entorno de bajo riesgo, lo que significa que un error no tendrá consecuencias graves (por ejemplo, un sistema de recomendación de películas) o que el método ya ha sido ampliamente estudiado y evaluado (por ejemplo, reconocimiento óptico de caracteres). La necesidad de interpretabilidad surge cuando **no es suficiente obtener la predicción (el qué), sino que el modelo también debe explicar cómo llegó a la predicción (el por qué)**.
* Además de eso, la interpretabilidad es importante en ciertas circunstancias particulares:
  1. En ciertas circunstancias, las personas no se conforman con el qué sino que su curiosidad o su necesidad hace que necesiten saber cómo funcionan ciertas cosas.
  2. En algunos casos, el algoritmo es utilizado por una disciplina científica que busca agregar conocimiento al mundo. En ese caso, no es la predicción lo que interesa sino directamente el algoritmo, es decir que es imprescindible interpretarlo.
  3. Los algoritmos pueden tener sesgos, en particular en contra de ciertas minorías. La aceptabilidad social de un algoritmo depende de su interpretabilidad.
  4. Los modelos solo pueden ser inspeccionados y auditados cuando son interpretables.
* Por el contrario, la interpretabilidad no es necesaria cuando el modelo tiene un riesgo muy bajo al equivocarse, cuando el problema está muy estudiado y hay muchos años de experiencia en el tema, o bien cuando las personas conociendolo podrían modificar su comportamiento para alterar el resultado.
* La **interpretabilidad intrínseca** se refiere a los modelos de aprendizaje automático que se consideran interpretables debido a su estructura simple, como árboles de decisión cortos o modelos lineales dispersos. La **interpretabilidad post hoc** se refiere a la aplicación de métodos de interpretación después del entrenamiento modelo.
* Un método de interpretación puede tener múltiples resultados: estadísticos de resumen, visualización de los atributos, elementos internos del modelo, ejemplos con datos reales. La interpretación también puede ser propia del modelo (como los coeficientes de la regresión) o general (pares de entrada y salida).
* Alcance de la interpretabilidad: Preguntas en las que valdría la pena detenerse.
  1. ¿Cómo crea el algoritmo el modelo? Esto requiere conocimiento del algoritmo, y no sólo de los datos.
  2. ¿Cómo hace predicciones el modelo entrenado? Entender cómo, a través de las variables y los pesos, se pueden generar las predicciones
  3. ¿Cómo afectan partes del modelo a las predicciones? Entender los pesos, a veces son contraintuitivos.
  4. ¿Por qué el modelo hizo una cierta predicción para una instancia?
  5. ¿Por qué el modelo hizo predicciones específicas para un grupo de instancias?

## 3. Modelos
* Un **modelo es lineal** si la asociación entre features y target se modela linealmente.
* Un modelo con restricciones de monotonicidad asegura que la relación entre una característica y el resultado objetivo siempre vaya en la misma dirección en todo el rango de la feature: un aumento en el valor de la característica siempre conduce a un aumento o siempre a una disminución en el objetivo 
* Algunos modelos pueden incluir automáticamente interacciones entre características para predecir el resultado objetivo.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.1.png"> </p>

### 1. Regresión lineal
* En este modelo, el resultado previsto de una instancia es una **suma ponderada de sus características p.**
* Las betas (β) representan los pesos o coeficientes de las características aprendidas. El primer peso en la suma (β0) se llama intercepción y no se multiplica con una característica. El épsilon (ϵ) es el error que cometemos, la diferencia entre la predicción y el resultado real. 
* Se supone que estos errores siguen una distribución gaussiana centrada en torno al 0, lo que significa que **cometemos muchos errores en direcciones negativas y positivas, muchos pequeños y pocos grandes**. Los pesos estimados vienen con intervalos de confianza. Un intervalo de confianza es un rango para la estimación de peso que cubre el peso "verdadero" con una cierta confianza. 
* Si el modelo es el modelo "correcto" depende de si las relaciones en los datos cumplen ciertos supuestos:
  * Linealidad: La predicción debe ser una combinación lineal de características. Los efectos lineales son fáciles de cuantificar y describir. Son aditivos, por lo que es fácil separar los efectos. 
  * Normalidad: Se supone que la esperanza del target condicionada a los features incluidos en la regresión sigue una distribución normal. Si se viola esta suposición, los intervalos de confianza estimados de los pesos de las características no son válidos.
Homocedasticidad (varianza constante): Se supone que la varianza del término de error es constante en todo el espacio de features. Esta suposición a menudo se viola en la realidad. 
  * Independencia: Se supone que cada observación es independiente de cualquier otra. 
  * Características fijas: Las features de entrada se consideran fijas, lo que implica que están libres de errores de medición. 
  * Ausencia de multicolinealidad: No desea características fuertemente correlacionadas, porque esto arruina la estimación de los pesos. 
* **En una situación en la que dos características están fuertemente correlacionadas, se vuelve problemático estimar los pesos porque los efectos de la característica son aditivos y se vuelve indeterminable a cuál de las características correlacionadas atribuir los efectos.**
* El peso de las variables se interpreta por la magnitud de sus beta, pero su importancia por su estimador t, que divide ese beta por el desvío estándar: la intuición es que una variable será más importante cuanto más grande sea el beta pero también cuánto más seguro estemos que ese es el valor real (o sea, más chico el desvío estándar). 
* **El R2 es la proporción de la variabilidad del target que el modelo explica en los datos usados para modelar**, y el R2 ajustado es una medida similar pero que considera la cantidad de variables: ![R^2_{adj} = R^2 * (1-R^2) * \frac{n-1}{n-p-1}](https://render.githubusercontent.com/render/math?math=R%5E2_%7Badj%7D%20%3D%20R%5E2%20*%20(1-R%5E2)%20*%20%5Cfrac%7Bn-1%7D%7Bn-p-1%7D) (para knitear la fórmula ayudó _[esto](https://www.latex4technics.com/)_ y _[esto](https://alexanderrodin.com/github-latex-markdown/)_).
* Las formas de visualizar el valor de los coeficientes puede ser con una especie de boxplot por feature donde cada barra esté centrada en el coeficiente y tenga la amplitud de su desvío estándar (weight plot), o con un boxplot por variable aplicada, donde cada valor de la variable en la muestra se multiplica por el coeficiente estimado y entonces se grafica en forma de boxplot los efectos reales de la variable sobre el target (effect plot). En este último gráfico podría señalarse, por ejemplo, con un color el valor de una observación particular en cada variable para entender, en ese caso, cuál es el valor predicho.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.2.png"> </p>
* A juzgar por lo que debe considerarse como una buena explicación ML, **los modelos lineales no crean las mejores explicaciones**. Son contrastables, pero el valor de referencia sobre el cual tienen interpretación los pesos es un caso en el que todas las variables numéricas son cero y, en forma arbitraria, las categóricas están en sus basales. Ese caso no suele ser real. 
* En ocasiones las variables se **centran en torno de la media**, para que el valor de referencia sea ‘el caso típico’. Esto también podría ser un punto de datos inexistente, pero al menos podría ser más probable o más significativo. Los modelos lineales crean explicaciones verdaderas, siempre que la ecuación lineal sea un modelo apropiado para la relación entre características y resultados. 
* Cuantas más no linealidades e interacciones haya, menos preciso será el modelo lineal y menos sinceras serán las explicaciones.
* Si la interpretabilidad crece cuando las variables son menos, se vuelven útiles métodos como **Lasso**, que realiza la selección de características y la regularización de los pesos de las características seleccionadas. Este modelo incorpora un término de penalización por los valores de los coeficientes, cosa que hace que muchos se ajusten a 0. El **parámetro lambda de penalización** dirá qué tanto se penalizará por cada peso: cuanto más grande sea, cada vez menos características reciben una estimación de peso diferente de cero.
<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.3.png"> </p>
A la izquierda del gráfico, el parámetro de penalización es muy chico y por lo tanto muchas variables (12) tienen un peso distinto de cero, por eso hay muchas líneas. En cuanto se incrementa el parámetro, más variables van hacia 0. El valor óptimo de lambda se elige muchas veces con validación cruzada. Lasso no es el único medio para reducir la cantidad de features, sino que hay otros como la selección manual, el umbral de correlación (sólo variables que correlacionan en cierta forma con el target) o el forward o backward selection (empezar con una sola o con todas las features, e ir recortando o agregando nuevas y viendo el impacto sobre el R2).
* *En conclusión, el modelo de regresión lineal es muy usado, por lo que en general es aceptado para el modelado predictivo y hacer inferencia. Matemáticamente es sencillo estimar los pesos y tiene la garantía de encontrar pesos óptimos (siempre y cuando los datos cumplen con los supuestos). Junto con los pesos se obtienen intervalos de confianza sustentados por una sólida espalda estadística. Como ventaja, el modelo sirve como base para muchas extensiones, como GLM. Sin embargo, los modelos de regresión lineal solo pueden representar relaciones lineales, es decir, una suma ponderada de las features de entrada. Cada no linealidad o interacción debe ser hecha a mano y definida explícitamente al modelo como una característica de entrada. Como vimos, además, la interpretación de un coeficiente en particular puede ser contraintuitiva, porque es en el contexto del modelo y depende de todas las demás características.*

### 2. Regresión logística
La regresión lineal falla para clasificación, por el hecho de que su salida no son probabilidade spor lo que no está acotada entre el 0 y el 1. Además, el elemento necesario para la clasificación (el umbral a partir del cual se clasificará de una u otra manera) no puede asumirse como significativo, ni en una clase ni en múltiples. Para subsanar esto es que se usa la regresión logística (binomial cuando es para dos clases, multinomial cuando son más), buscando acotar el output de la función entre 0 y 1. La función logística (![\frac{1}{1+e^{-\eta}}](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ceta%7D%7D)) subsana esto, tomando un valor siempre entre 0 y 1, coincidente con 0,5 cuando η es 0. La probabilidad, entonces, el lado izquierdo de la ecuación reemplazando η por la fórmula con el intercepto y las variables multiplicadas por su coeficiente. 

La interpretación de los pesos en la regresión logística difiere de la interpretación de los pesos en la regresión lineal, ya que el resultado en la regresión logística es una probabilidad entre 0 y 1. Los pesos ya no influyen en la probabilidad linealmente. La suma ponderada se transforma mediante la función logística en una probabilidad. Por lo tanto, necesitamos reformular la ecuación para la interpretación de modo que solo el término lineal esté en el lado derecho de la fórmula: ![\log \frac{P(y=1)}{P(y=0)} = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n](https://render.githubusercontent.com/render/math?math=%5Clog%20%5Cfrac%7BP(y%3D1)%7D%7BP(y%3D0)%7D%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1%20x_1%20%2B%20...%20%2B%20%5Cbeta_n%20x_n). El término dentro del logaritmo son las chances (odds), por lo cual el modelo de regresión logística es un modelo lineal del logaritmo de las odds. Un cambio en un feature en una unidad cambia el ratio de los logaritmos de los odds, en forma lineal al peso de esa feature. Si las odds son 2, significa que la probabilidad de y = 1 es el doble de y = 0. Si tiene un peso de 0.7, al aumentar la característica respectiva en una unidad multiplica las probabilidades por e^(0.7) (aproximadamente 2) y las odds cambian a 4. Es importante en la regresión logística comprender el valor basal, que es el que toman las variables cuando son 0. El valor del intercepto es, en este caso, útil ya que cuando todas las variables numéricas son cero y las categóricas están en su categoría basal, las odds estimadas son e^β0.

Muchos de los pros del modelo lineal de regresión aplican también al modelo logístico de regresión. Como ventaja adicional, por sobre otros modelos de este tipo, es que no solo devuelve la clasificación sino la probabilidad. Es distinto saber que una observación tiene una probabilidad del 99% para una clase, que saber que tiene probabilidad del 51%. Como desventaja particular aparece la pérdida de poder de interpretación de los pesos. Una situación adicional es que, si hay una característica que separe perfectamente las dos clases, el modelo de regresión logística ya no puede ser entrenado. Esto se debe a que el peso de esa característica no convergería, porque el peso óptimo sería infinito. Uno podría pensar que esto arruina el problema, pero en caso de que una característica separe perfectamente las dos clases, no es necesario un modelo ML.

### 3. GLM, GAM y otros modelos
La característica por excelencia del modelo de regresión lineal es que la predicción se modela como una suma de características. Además, requiere de supuestos que muchas veces no se cumplen: puede suceder que E(Y|X) no se distribuya normalmente, por ejemplo. Hay muchos casos en los que la regresión es inútil por diferentes motivos:
  * Si los errores son heterocedásticos o hay outliers, se puede usar regresión robusta.
  * Si lo que se busca es predecir el tiempo hasta la ocurrencia de un evento, se pueden usar diferentes modelos de supervivencia.
  * Si lo que se quiere es predecir una cantidad, se puede usar una regresión de Poisson.

Pero hay otros casos en los que la cuestión radica en el ajuste de los datos a los supuestos propios de la regresión, por lo que puede resolverse relajando la naturaleza de la regresión simple. Recordemos los supuestos clave sobre los datos, que son tres:

<p align="center"> <img src="https://github.com/fjf-arg/books/blob/master/images/3.4.png"> </p>

Los requisitos del modelo lineal pueden mostrarse como en el lado izquierdo, y sus casos opuestos como en el derecho. El primer caso sería el caso de no normalidad, y se resuelve con GLM. El segundo sería el caso de variables que interactúan, y se soluciona incorporando interacciones manualmente. El último sería el de relaciones no lineales, que se resuelve con GAM.

#### 3.i. Resultados no gaussianos - GLM

Los casos en los que la variable respuesta (condicionada a los valores de las features) no sigue una distribución normal pueden ser múltiples: variable respuesta de un tipo determinado (conteo, categoría, cantidad de tiempo) o simplemente una variable numérica, pero con una forma de distribución diferente (como las que se refieren al ingreso). El concepto central de cualquier GLM es: mantener la suma ponderada de las características, pero permitir distribuciones de resultados no gaussianas y conectar la media esperada de esta distribución y la suma ponderada a través de una función posiblemente no lineal. La regresión logística es un ejemplo: supone una distribución de Bernoulli para el target y vincula la media esperada y la suma ponderada utilizando la función logística. 
La expresión, g(E_Y(y|x)) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n, contiene tres elementos: la parte derecha que es igual que en la regresión lineal (llamada predictor), y del lado izquierdo una función link g() y una E(y|x), que debe tener forma exponencial. Hay muchas distribuciones [de tipo exponencial](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions/). Si la distribución es gaussiana y la función es identidad, estamos en la regresión lineal simple; si la distribución es de Bernoulli y la función es logit estamos ante una regresión logística. Como en la logística, los GLM cambian su interpretación con respecto a los coeficientes, que en algunos casos pueden ser más sencillos de interpretar y en otros más complejos.

#### 3.ii. Interacciones

En ocasiones, el efecto de una feature sobre la respuesta no es independiente del valor de otras features. En estos casos es útil incluir interacciones entre las variables, es decir crear nuevas variables dadas por combinaciones de ambas. En el caso de las variables numéricas el método es sencillo, y es el de multiplicar una por otra. Interpretando una salida de un modelo construido así, el efecto de cada variable sobre la respuesta puede dejar de ser lineal, sino que ser diferente según el valor (o la activación o no) de otra variable. De este modo, la relación deja de ser estrictamente lineal.

#### 3.iii. No linealidades

La linealidad en los modelos significa que no importa qué valor tenga una observación en una característica particular, aumentar el valor en una unidad siempre tiene el mismo efecto en el resultado previsto. Esto claramente no se verifica en la práctica, y posiblmente sea el menos realista de los supuestos. Existen tres formas de subsanar esta característica de los datos (que puede observarse cuando la nube de puntos de la variable con respecto al target no dibuja una línea).
  * Transformación de variables: Aplicar el logaritmo, suavizando la relación entre las variables. Esto hace que la relación deje de ser constante, interpretándose como "Si el logaritmo de la característica se incrementa en uno, la predicción se incrementa en el peso correspondiente".
  * Categorización de variables: Generar variables específicas para cuando el valor está en ciertos rangos, lo cual 'des-linealiza' la relación. Sin embargo, es muy probable caer en overfitting y no está demasiado claro como discretizar una variable contínua.
  * GLM: Sin duda, la solución más completa es la de aceptar la no-linealidad y trabajar con ello: relajar la restricción de una suma de pesos lineales, para trabajar con una suma de funciones: ![g(E_Y(y|x)) = \beta_0 + f_1(x_1) + ... + f_n(x_n)](https://render.githubusercontent.com/render/math?math=g(E_Y(y%7Cx))%20%3D%20%5Cbeta_0%20%2B%20f_1(x_1)%20%2B%20...%20%2B%20f_n(x_n)). GAM sigue siendo una suma de efectos de features, pero tiene la opción de permitir relaciones no lineales entre algunas features y el target. Lo importante en este caso es cómo aproximar la función, para lo que se usan splines, curvas diferenciables definidas en porciones mediante polinomios. La salida del modelo asignará pesos a cada spline. 
  * GLM: Sin duda, la solución más completa es la de aceptar la no-linealidad y trabajar con ello: relajar la restricción de una suma de pesos lineales, para trabajar con una suma de funciones: ![g(E_Y(y|x)) = \beta_0 + f_1(x_1) + ... + f_n(x_n)](https://render.githubusercontent.com/render/math?math=g(E_Y(y%7Cx))%20%3D%20%5Cbeta_0%20%2B%20f_1(x_1)%20%2B%20...%20%2B%20f_n(x_n)). GAM sigue siendo una suma de efectos de features, pero tiene la opción de permitir relaciones no lineales entre algunas features y el target. Lo importante en este caso es cómo aproximar la función, para lo que se usan splines, curvas diferenciables definidas en porciones mediante polinomios. La salida del modelo asignará pesos a cada spline. 
  
En síntesis, las extensiones del modelo lineal son enormes, y resuelven casi todos los problemas que pueden tener los datos con respecto a los modelos lineales. Muchos investigadores y profesionales de la industria tienen mucha experiencia con modelos lineales y los métodos son aceptados en muchas comunidades como status quo para el modelado. Los softwares estadísticos generalmente tienen interfaces realmente buenas para adaptarse a GLM, GAM y modelos lineales más especiales. Suponiendo que los modelos lineales son altamente interpretables pero a menudo no se ajustan a la realidad, las extensiones descritas en este capítulo ofrecen una buena manera de lograr una transición suave a modelos más flexibles, al tiempo que conservan algo de la capacidad de interpretación. Sin embargo, como se dijo, la mayoría de las modificaciones del modelo lineal hacen que el modelo sea menos interpretable. Cualquier función de enlace (en un GLM) que no sea la función de identidad complica la interpretación; las interacciones también complican la interpretación; los efectos de características no lineales son menos intuitivos (como la transformación logarítmica) o ya no se pueden resumir en un solo número (por ejemplo, funciones de spline).

### 4. Árboles de decisión
La regresión lineal y los modelos de regresión logística fallan en situaciones donde la relación entre las características y el resultado es no lineal o donde las características interactúan entre sí. Los modelos basados en árboles dividen los datos varias veces de acuerdo con ciertos valores de corte en las características. A través de la división, se crean diferentes subconjuntos del conjunto de datos, y cada observación pertenece a un subconjunto. Los subconjuntos finales se denominan nodos terminales o de hoja y los subconjuntos intermedios se denominan nodos internos o nodos divididos. Para predecir el resultado en cada nodo hoja, se utiliza el resultado promedio de los datos de entrenamiento en este nodo. Los árboles se pueden usar para clasificación y regresión. La fórmula que describe los árboles es ![\hat{y} = \hat{f}(x) = \sum_{m=1}^{M} c_mI \left\lbrace x \varepsilon R_m \right\rbrace](https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D%20%3D%20%5Chat%7Bf%7D(x)%20%3D%20%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20c_mI%20%5Cleft%5Clbrace%20x%20%5Cvarepsilon%20R_m%20%5Cright%5Crbrace). Cada observacion cae exactamente en un nodo hoja (subconjunto Rm). I (y lo que tiene dentro) es la función de identidad que retorna 1 si x está en el subset Rm y 0 en otros casos. Si una instancia cae en un nodo hoja Ri, el resultado previsto es ci, promedio de todas las instancias de entrenamiento en ese nodo. Para la determinación de los puntos de corte, el elemento central es el algoritmo CART:

¿Pero de dónde vienen los subconjuntos? Esto es bastante simple: CART toma una función y determina qué punto de corte minimiza la varianza de y en regresión (es decir, qué tanto los valores y en un nodo se distribuyen alrededor de su valor medio) o el índice de Gini en clasificación (qué tan "impuro" es un nodo, si todas las clases tienen la misma frecuencia, el nodo es impuro, si solo hay una clase presente, es máximo puro). La varianza y el índice de Gini se minimizan cuando las observaciones en los nodos tienen valores muy similares para y. Como consecuencia, el mejor punto de corte hace que los dos subconjuntos resultantes sean lo más diferentes posible con respecto al resultado objetivo. El algoritmo continúa esta búsqueda y división recursivamente en ambos nodos nuevos hasta que se alcanza un criterio de detención. Los criterios posibles son: Un número mínimo de observaciones que deben estar en un nodo antes de la división, o el número mínimo de observaciones que deben estar en un nodo terminal.

La interpretación es simple: comenzando desde el nodo raíz, vas a los siguientes nodos y los bordes te dicen qué subconjuntos estás mirando. Una vez que llegue al nodo hoja, el nodo le indica el resultado predicho. La importancia general de una feature en un árbol de decisión nos dice cuánto ayudó una característica a mejorar la pureza de todos los nodos, y se puede calcular de la siguiente manera: revise todas las divisiones para las que se utilizó la característica y mida cuánto ha reducido la varianza o el índice de Gini en comparación con el nodo primario. La suma de todas las importancias se escala a 100. Esto significa que cada importancia puede interpretarse como parte de la importancia general del modelo.

En síntesis, la estructura de árbol es ideal para capturar interacciones entre características en los datos. La estructura de árbol también tiene una visualización natural, con sus nodos y bordes. La estructura de árbol invita automáticamente a pensar en los valores pronosticados para instancias individuales como contrafactuales: “Si una entidad hubiera sido mayor / menor que el punto de división, la predicción habría sido y1 en lugar de y2". Además, a diferencia de los modelos lineales, nunca hay necesidad de transformar características. Sin embargo, los árboles no pueden lidiar con relaciones lineales. Esto va de la mano con la falta de suavidad. Los cambios leves en la función de entrada pueden tener un gran impacto en el resultado previsto, lo que generalmente no es deseable. Los árboles también son bastante inestables. Algunos cambios en el conjunto de datos de entrenamiento pueden crear un árbol completamente diferente. Los árboles de decisión son muy interpretables, siempre que sean cortos. El número de nodos terminales aumenta rápidamente con la profundidad.

### 5. Reglas de decisión
Una regla de decisión es una cláusula IF-THEN. Su estructura IF-THEN se asemeja semánticamente al lenguaje natural y a la forma en que pensamos, siempre que la condición se construya a partir de características inteligibles. La utilidad de una regla de decisión generalmente se resume en dos números: Soporte y precisión. El soporte es el porcentaje de instancias a las que se aplica la condición de una regla se denomina soporte. La precisión, en cambio, es una medida de la precisión de la regla al predecir la clase correcta para las instancias a las que se aplica la condición de la regla. Por lo general, existe un trade off entre precisión y soporte: al agregar más características a la condición, podemos lograr una mayor precisión, pero perdemos soporte. Una lista de decisiones introduce un orden a las reglas de decisión: si la condición de la primera regla es verdadera para una instancia, usamos la predicción de la primera regla; si no, pasamos a la siguiente regla y verificamos si corresponde y así sucesivamente. Un conjunto de decisiones se asemeja a una democracia de las reglas, excepto que algunas reglas pueden tener un mayor poder de voto. La regla predeterminada es la regla que se aplica cuando no se aplica ninguna otra regla. Hay muchas maneras de aprender reglas de los datos y este libro está lejos de abarcarlas todas. Este capítulo te muestra tres de ellos.

  * OneR aprende las reglas de una sola feature. OneR se caracteriza por su simplicidad, interpretabilidad y su uso como benchmark. De todas las características, OneR selecciona la que lleva más información sobre el resultado de interés y crea reglas de decisión a partir de esta característica. El algoritmo es simple y rápido: discretiza las características continuas eligiendo los intervalos apropiados. Para cada característica, cree una tabla cruzada entre los valores de la característica y el resultado (categórico), de la que crea la regla de decisión. Calcula el error total de las reglas para la función, y luego selecciona la función con el error total más pequeño.
  
  * Sequential covering es un procedimiento general que aprende de forma iterativa las reglas y elimina las observaciones cubiertos por la nueva regla. Este procedimiento es utilizado por muchos algoritmos de aprendizaje de reglas. La idea es simple: primero, encuentre una buena regla que se aplique a algunas de las observaciones. Elimine ellas, independientemente de si los puntos se clasifican correctamente o no. Repita el aprendizaje de reglas y la eliminación de puntos cubiertos con los puntos restantes hasta que no queden más puntos o se cumpla otra condición de detención. El resultado es una lista de decisiones. Este enfoque de aprendizaje de reglas repetido y eliminación de observaciones cubiertas se denomina "separar y conquistar". Para múltiples clases, el enfoque debe ser modificado: el algoritmo de cobertura secuencial comienza con la clase menos común, aprende una regla para ello, elimina todas las instancias cubiertas, luego pasa a la segunda clase menos común y así sucesivamente. Existen muchos algoritmos para definir cuál es la regla que se creará, porque este proceso comienza por una regla.
  
  * Bayesian Rule Lists combinan patrones frecuentes pre-minados en una lista de decisiones utilizando estadísticas bayesianas. Un patrón frecuente es la frecuente (co) aparición de valores de features. Como un paso de preprocesamiento para el algoritmo BRL, utilizamos las características y extraemos patrones que ocurren con frecuencia en conjunto. Existen muchos algoritmos para encontrar patrones tan frecuentes, por ejemplo, Apriori o FP-Growth. El objetivo del algoritmo BRL es aprender una lista de decisiones precisa utilizando una selección de las condiciones predeterminadas, al tiempo que prioriza listas con pocas reglas y condiciones cortas. El algoritmo comienza con patrones de valor de característica de pre-minería con el algoritmo FP-Growth. BRL hace una serie de suposiciones sobre la distribución del target y la distribución de los parámetros que definen la distribución del target (esa es la estadística bayesiana). Las estimaciones en las estadísticas bayesianas siempre son un poco complicadas, porque generalmente no podemos calcular directamente la respuesta correcta, pero tenemos que dibujar candidatos, evaluarlos y actualizar nuestras estimaciones posteriores utilizando Cadenas de Markov. Los autores de BRL proponen primero dibujar una lista de decisión inicial y luego modificarla iterativamente para generar muestras de listas de decisión a partir de la distribución posterior de las listas (Markov).

En síntesis, las reglas IF-THEN son fáciles de interpretar. Son probablemente el más interpretable de los modelos interpretables. Las reglas de decisión pueden ser tan expresivas como los árboles de decisión, a la vez que son más compactas. La predicción con las reglas IF-THEN es rápida, ya que solo se necesitan verificar unas pocas declaraciones binarias para determinar qué reglas se aplican. Las reglas de decisión son robustas frente a las transformaciones monótonas de las características de entrada, porque solo cambia el umbral en las condiciones. También son robustos frente a los valores atípicos, ya que solo importa si una condición se aplica o no. Como contras, se puede ver que la investigación y la literatura para las reglas IF-THEN se enfoca en la clasificación y descuida casi por completo la regresión. A menudo, las características también tienen que ser categóricas. Eso significa que las características numéricas deben clasificarse si desea utilizarlas: la categorización de características continuas es un problema no trivial que a menudo se descuida. Las reglas de decisión son malas al describir relaciones lineales entre características y resultados. Ese es un problema que comparten con los árboles de decisión. Los árboles de decisión y las reglas solo pueden producir funciones de predicción escalonadas, donde los cambios en la predicción siempre son pasos discretos y nunca curvas suaves. Esto está relacionado con el problema de que las entradas tienen que ser categóricas.

### 5. Rulefit
El algoritmo RuleFit de aprende modelos lineales dispersos que incluyen efectos de interacción detectados automáticamente en forma de reglas de decisión. RuleFit aprende un modelo lineal disperso con las features originales y también con una serie de características nuevas que son reglas de decisión. Estas nuevas características capturan interacciones entre las features originales. RuleFit genera automáticamente estas características a partir de los árboles de decisión. Cada ruta a través de un árbol se puede transformar en una regla de decisión combinando las decisiones divididas en una regla. Cada árbol se descompone en reglas de decisión que se utilizan como características adicionales en un modelo de regresión lineal disperso, que utiliza Lasso. La interpretación es análoga a los modelos lineales: el resultado predicho cambia tanto como los coeficientes, si la feature cambia en una unidad, siempre que todas las demás características permanezcan sin cambios. 

RuleFit agrega automáticamente interacciones de features a modelos lineales. Por lo tanto, resuelve el problema de los modelos lineales donde las interacciones deben agregarse manualmente. Además, las reglas creadas son fáciles de interpretar, porque son reglas de decisión binarias. La regla se aplica a una observación o no. La buena interpretación solo se garantiza si el número de condiciones dentro de una regla no es demasiado grande. Una regla con 1 a 3 condiciones me parece razonable. Esto significa una profundidad máxima de 3 para los árboles en el conjunto de árboles.

### 7. Otros
La lista de modelos interpretables está en constante crecimiento y de tamaño desconocido. Incluye modelos simples como modelos lineales, árboles de decisión y bayesianos, pero también modelos más complejos que combinan o modifican modelos de aprendizaje automático no interpretables para hacerlos más interpretables.

En el modelo Naive Bayes, por ejemplo, la probabilidad condicional de una clase es la probabilidad de clase multiplicada por la probabilidad de cada feature dada la clase, normalizada por Z. Esta fórmula puede derivarse utilizando el teorema de Bayes. Se trata de un modelo interpretable debido al supuesto de independencia: está muy claro para cada característica cuánto contribuye a una determinada predicción de clase, ya que podemos interpretar la probabilidad condicional.

El método de vecino más cercano se puede usar para regresión y clasificación y utiliza los vecinos más cercanos de una observación para la predicción. Para la clasificación, el método vecino más cercano a k asigna la clase más común de los vecinos más cercanos de una instancia. Para la regresión, toma el promedio del resultado de los vecinos. Este modelo difiere de los otros modelos interpretables presentados en este libro porque es un algoritmo de aprendizaje basado en instancias.
